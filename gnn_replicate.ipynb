{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gnn_replicate.ipynb","provenance":[],"authorship_tag":"ABX9TyN3fCo+0DDDUwOL8LiwH9Lx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m7I2PDGdguQw"},"source":["# Setup\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B243Ji0Vz8zr","executionInfo":{"status":"ok","timestamp":1608008784342,"user_tz":480,"elapsed":42012,"user":{"displayName":"Xiao Liang","photoUrl":"","userId":"15028243839916116675"}},"outputId":"e1cc8272-905d-46f1-d171-470574e10498"},"source":["from google.colab import drive\n","drive.mount('/gdrive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kkiii4LI0GFa"},"source":["!bash pip install -r /gdrive/MyDrive/CSE\\ 490G/Final\\ Project/requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2cu9_Tu0FHo"},"source":["import graph_nets as gn\n","import sonnet as snt\n","import functools\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","import collections\n","import json\n","import os\n","import pickle\n","import tree\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","from absl import app\n","from absl import flags\n","from absl import logging\n","from sklearn import neighbors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aoJPRZRrCmQO"},"source":["# Utils (connectivity, data loading, noise)"]},{"cell_type":"markdown","metadata":{"id":"s2trO4wYj4yH"},"source":["### data loading"]},{"cell_type":"code","metadata":{"id":"8DdAIbmdjmRK"},"source":["# Create a description of the features.\n","_FEATURE_DESCRIPTION = {\n","    'position': tf.io.VarLenFeature(tf.string),\n","}\n","\n","_FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT = _FEATURE_DESCRIPTION.copy()\n","_FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT['step_context'] = tf.io.VarLenFeature(\n","    tf.string)\n","\n","_FEATURE_DTYPES = {\n","    'position': {\n","        'in': np.float32,\n","        'out': tf.float32\n","    },\n","    'step_context': {\n","        'in': np.float32,\n","        'out': tf.float32\n","    }\n","}\n","\n","_CONTEXT_FEATURES = {\n","    'key': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n","    'particle_type': tf.io.VarLenFeature(tf.string)\n","}\n","\n","\n","def convert_to_tensor(x, encoded_dtype):\n","  if len(x) == 1:\n","    out = np.frombuffer(x[0].numpy(), dtype=encoded_dtype)\n","  else:\n","    out = []\n","    for el in x:\n","      out.append(np.frombuffer(el.numpy(), dtype=encoded_dtype))\n","  out = tf.convert_to_tensor(np.array(out))\n","  return out\n","\n","\n","def parse_serialized_simulation_example(example_proto, metadata):\n","  \"\"\"Parses a serialized simulation tf.SequenceExample.\n","\n","  Args:\n","    example_proto: A string encoding of the tf.SequenceExample proto.\n","    metadata: A dict of metadata for the dataset.\n","\n","  Returns:\n","    context: A dict, with features that do not vary over the trajectory.\n","    parsed_features: A dict of tf.Tensors representing the parsed examples\n","      across time, where axis zero is the time axis.\n","\n","  \"\"\"\n","  if 'context_mean' in metadata:\n","    feature_description = _FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT\n","  else:\n","    feature_description = _FEATURE_DESCRIPTION\n","  context, parsed_features = tf.io.parse_single_sequence_example(\n","      example_proto,\n","      context_features=_CONTEXT_FEATURES,\n","      sequence_features=feature_description)\n","  for feature_key, item in parsed_features.items():\n","    convert_fn = functools.partial(\n","        convert_to_tensor, encoded_dtype=_FEATURE_DTYPES[feature_key]['in'])\n","    parsed_features[feature_key] = tf.py_function(\n","        convert_fn, inp=[item.values], Tout=_FEATURE_DTYPES[feature_key]['out'])\n","\n","  # There is an extra frame at the beginning so we can calculate pos change\n","  # for all frames used in the paper.\n","  position_shape = [metadata['sequence_length'] + 1, -1, metadata['dim']]\n","\n","  # Reshape positions to correct dim:\n","  parsed_features['position'] = tf.reshape(parsed_features['position'],\n","                                           position_shape)\n","  # Set correct shapes of the remaining tensors.\n","  sequence_length = metadata['sequence_length'] + 1\n","  if 'context_mean' in metadata:\n","    context_feat_len = len(metadata['context_mean'])\n","    parsed_features['step_context'] = tf.reshape(\n","        parsed_features['step_context'],\n","        [sequence_length, context_feat_len])\n","  # Decode particle type explicitly\n","  context['particle_type'] = tf.py_function(\n","      functools.partial(convert_fn, encoded_dtype=np.int64),\n","      inp=[context['particle_type'].values],\n","      Tout=[tf.int64])\n","  context['particle_type'] = tf.reshape(context['particle_type'], [-1])\n","  return context, parsed_features\n","\n","\n","def split_trajectory(context, features, window_length=7):\n","  \"\"\"Splits trajectory into sliding windows.\"\"\"\n","  # Our strategy is to make sure all the leading dimensions are the same size,\n","  # then we can use from_tensor_slices.\n","\n","  trajectory_length = features['position'].get_shape().as_list()[0]\n","\n","  # We then stack window_length position changes so the final\n","  # trajectory length will be - window_length +1 (the 1 to make sure we get\n","  # the last split).\n","  input_trajectory_length = trajectory_length - window_length + 1\n","\n","  model_input_features = {}\n","  # Prepare the context features per step.\n","  model_input_features['particle_type'] = tf.tile(\n","      tf.expand_dims(context['particle_type'], axis=0),\n","      [input_trajectory_length, 1])\n","\n","  if 'step_context' in features:\n","    global_stack = []\n","    for idx in range(input_trajectory_length):\n","      global_stack.append(features['step_context'][idx:idx + window_length])\n","    model_input_features['step_context'] = tf.stack(global_stack)\n","\n","  pos_stack = []\n","  for idx in range(input_trajectory_length):\n","    pos_stack.append(features['position'][idx:idx + window_length])\n","  # Get the corresponding positions\n","  model_input_features['position'] = tf.stack(pos_stack)\n","\n","  return tf.data.Dataset.from_tensor_slices(model_input_features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"plvXvp6Yj94Y"},"source":["### graph connectivity"]},{"cell_type":"code","metadata":{"id":"76EE3VCqkCJG"},"source":["def _compute_connectivity(positions, radius):\n","  \"\"\"Get the indices of connected edges with radius connectivity.\n","\n","  Args:\n","    positions: Positions of nodes in the graph. Shape:\n","      [num_nodes_in_graph, num_dims].\n","    radius: Radius of connectivity.\n","\n","  Returns:\n","    senders indices [num_edges_in_graph]\n","    receiver indices [num_edges_in_graph]\n","\n","  \"\"\"\n","  tree = neighbors.KDTree(positions)\n","  receivers_list = tree.query_radius(positions, r=radius)\n","  num_nodes = len(positions)\n","  senders = np.repeat(range(num_nodes), [len(a) for a in receivers_list])\n","  receivers = np.concatenate(receivers_list, axis=0)\n","  return senders, receivers\n","\n","\n","def _compute_connectivity_for_batch(positions, n_node, radius):\n","  \"\"\"`compute_connectivity` for a batch of graphs.\n","\n","  Args:\n","    positions: Positions of nodes in the batch of graphs. Shape:\n","      [num_nodes_in_batch, num_dims].\n","    n_node: Number of nodes for each graph in the batch. Shape:\n","      [num_graphs in batch].\n","    radius: Radius of connectivity.\n","\n","  Returns:\n","    senders indices [num_edges_in_batch]\n","    receiver indices [num_edges_in_batch]\n","    number of edges per graph [num_graphs_in_batch]\n","\n","  \"\"\"\n","\n","  # TODO(alvarosg): Consider if we want to support batches here or not.\n","  # Separate the positions corresponding to particles in different graphs.\n","  positions_per_graph_list = np.split(positions, np.cumsum(n_node[:-1]), axis=0)\n","  receivers_list = []\n","  senders_list = []\n","  n_edge_list = []\n","  num_nodes_in_previous_graphs = 0\n","\n","  # Compute connectivity for each graph in the batch.\n","  for positions_graph_i in positions_per_graph_list:\n","    senders_graph_i, receivers_graph_i = _compute_connectivity(\n","        positions_graph_i, radius)\n","\n","    num_edges_graph_i = len(senders_graph_i)\n","    n_edge_list.append(num_edges_graph_i)\n","\n","    # Because the inputs will be concatenated, we need to add offsets to the\n","    # sender and receiver indices according to the number of nodes in previous\n","    # graphs in the same batch.\n","    receivers_list.append(receivers_graph_i + num_nodes_in_previous_graphs)\n","    senders_list.append(senders_graph_i + num_nodes_in_previous_graphs)\n","\n","    num_nodes_graph_i = len(positions_graph_i)\n","    num_nodes_in_previous_graphs += num_nodes_graph_i\n","\n","  # Concatenate all of the results.\n","  senders = np.concatenate(senders_list, axis=0).astype(np.int32)\n","  receivers = np.concatenate(receivers_list, axis=0).astype(np.int32)\n","  n_edge = np.stack(n_edge_list).astype(np.int32)\n","\n","  return senders, receivers, n_edge\n","\n","\n","def compute_connectivity_for_batch_pyfunc(positions, n_node, radius):\n","  \"\"\"`_compute_connectivity_for_batch` wrapped in a pyfunc.\"\"\"\n","  senders, receivers, n_edge = tf.py_function(\n","      _compute_connectivity_for_batch,\n","      [positions, n_node, radius], [tf.int32, tf.int32, tf.int32])\n","  senders.set_shape([None])\n","  receivers.set_shape([None])\n","  n_edge.set_shape(n_node.get_shape())\n","  return senders, receivers, n_edge"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ixV3hdvkGVv"},"source":["### noise"]},{"cell_type":"code","metadata":{"id":"Mx9oPAQlkJxL"},"source":["# Lint as: python3\n","# pylint: disable=g-bad-file-header\n","# Copyright 2020 DeepMind Technologies Limited. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#    http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ============================================================================\n","\"\"\"Methods to calculate input noise.\"\"\"\n","\n","import tensorflow.compat.v1 as tf\n","\n","\n","def get_random_walk_noise_for_position_sequence(\n","    position_sequence, noise_std_last_step):\n","  \"\"\"Returns random-walk noise in the velocity applied to the position.\"\"\"\n","\n","  velocity_sequence = time_diff(position_sequence)\n","\n","  # We want the noise scale in the velocity at the last step to be fixed.\n","  # Because we are going to compose noise at each step using a random_walk:\n","  # std_last_step**2 = num_velocities * std_each_step**2\n","  # so to keep `std_last_step` fixed, we apply at each step:\n","  # std_each_step `std_last_step / np.sqrt(num_input_velocities)`\n","  # TODO(alvarosg): Make sure this is consistent with the value and\n","  # description provided in the paper.\n","  num_velocities = velocity_sequence.shape.as_list()[1]\n","  velocity_sequence_noise = tf.random.normal(\n","      tf.shape(velocity_sequence),\n","      stddev=noise_std_last_step / num_velocities ** 0.5,\n","      dtype=position_sequence.dtype)\n","\n","  # Apply the random walk.\n","  velocity_sequence_noise = tf.cumsum(velocity_sequence_noise, axis=1)\n","\n","  # Integrate the noise in the velocity to the positions, assuming\n","  # an Euler intergrator and a dt = 1, and adding no noise to the very first\n","  # position (since that will only be used to calculate the first position\n","  # change).\n","  position_sequence_noise = tf.concat([\n","      tf.zeros_like(velocity_sequence_noise[:, 0:1]),\n","      tf.cumsum(velocity_sequence_noise, axis=1)], axis=1)\n","\n","  return position_sequence_noise\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7dxwqHRCMG9"},"source":["# Graph Network"]},{"cell_type":"code","metadata":{"id":"S8wI6GD5kYzX"},"source":["def build_mlp(\n","    hidden_size: int, num_hidden_layers: int, output_size: int) -> snt.Module:\n","  \"\"\"Builds an MLP.\"\"\"\n","  return snt.nets.MLP(\n","      output_sizes=[hidden_size] * num_hidden_layers + [output_size])\n","\n","\n","class EncodeProcessDecode(snt.AbstractModule):\n","  \"\"\"Encode-Process-Decode function approximator for learnable simulator.\"\"\"\n","\n","  def __init__(\n","      self,\n","      latent_size: int,\n","      mlp_hidden_size: int,\n","      mlp_num_hidden_layers: int,\n","      num_message_passing_steps: int,\n","      output_size: int,\n","      name: str = \"EncodeProcessDecode\"):\n","    \"\"\"Inits the model.\n","\n","    Args:\n","      latent_size: Size of the node and edge latent representations.\n","      mlp_hidden_size: Hidden layer size for all MLPs.\n","      mlp_num_hidden_layers: Number of hidden layers in all MLPs.\n","      num_message_passing_steps: Number of message passing steps.\n","      output_size: Output size of the decode node representations as required\n","        by the downstream update function.\n","      name: Name of the model.\n","    \"\"\"\n","\n","    super().__init__(name=name)\n","\n","    self._latent_size = latent_size\n","    self._hidden_size = mlp_hidden_size\n","    self._num_mlp = mlp_num_hidden_layers\n","    self._num_message_passing_steps = num_message_passing_steps\n","    self._output_size = output_size\n","\n","    with self._enter_variable_scope():\n","      self._networks_builder()\n","\n","    # _build are required for a snt module\n","  def _build(self, input_graph: gn.graphs.GraphsTuple) -> tf.Tensor:\n","    latent_graph = self._encoder(input_graph)\n","    latent_graph = self.process_with_residual(latent_graph)\n","    return self._decoder(latent_graph)\n","  \n","  def generate_mlp_latent():\n","    output_sizes = [[self._hidden_size] * self.num_mlp + [self._latent_size]] # hidden_num of layers(size=hidden_size) plus an output layer\n","    mlp = snt.nets.MLP(output_sizes)\n","    return snt.Sequential([mlp, snt.LayerNorm()])\n","\n","  def generate_mlp_output():\n","    output_sizes = [[self._hidden_size] * self._num_mlp + [self._output_size]] # hidden_num of layers(size=hidden_size) plus an output layer\n","    mlp = snt.nets.MLP(output_sizes)\n","    return snt.Sequential([mlp, snt.LayerNorm()])\n","\n","  # a function that initialize encoder, processor and the decoder\n","  def _init_network(self):\n","    self._encoder = gn.modules.GraphIndependent(edge_model_fn=generate_mlp_latent\n","                                                , node_model_fn=generate_mlp_latent)\n","    self._processors = []\n","    for i in range(m):\n","      self._processors.append(gn.modules.InteractionNetwork(\n","              edge_model_fn=generate_mlp_latent,\n","              node_model_fn=generate_mlp_latent))\n","    self._decoder = generate_mlp_output()\n","\n","  # takes in an encoded graph run through m steps of processes with residual connection\n","  def process_with_residual(encoded):\n","    stride = self.residual_stride\n","    latent_graph = encoded\n","    prev_latent_graph = latent_graph;\n","    for step, processor in enumerate(self._processors):\n","      latent_graph = processsor(latent_graph)\n","      if step % stride == 0:\n","        if step != 0:\n","          latent_graph = latent_graph.replace(nodes=latent_graph.node+prev_latent_graph.node,\n","                                              edges=latent_graph.edges+prev_latent_graph.edges)\n","        prev_latent_graph = latent_graph\n","    return latent_graph"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvqLj_tfCTaU"},"source":["# Wrapper Class (pre & post processing)"]},{"cell_type":"code","metadata":{"id":"yFsA-EiEkgsW"},"source":["STD_EPSILON = 1e-8\n","\n","\n","class LearnedSimulator(snt.AbstractModule):\n","  \"\"\"Learned simulator from https://arxiv.org/pdf/2002.09405.pdf.\"\"\"\n","\n","  def __init__(\n","      self,\n","      num_dimensions,\n","      connectivity_radius,\n","      graph_network_kwargs,\n","      boundaries,\n","      normalization_stats,\n","      num_particle_types,\n","      particle_type_embedding_size,\n","      name=\"LearnedSimulator\"):\n","    super().__init__(name=name)\n","\n","    self._connectivity_radius = connectivity_radius\n","    self._num_particle_types = num_particle_types\n","    self._boundaries = boundaries\n","    self._normalization_stats = normalization_stats\n","    with self._enter_variable_scope():\n","      self._graph_network = EncodeProcessDecode(\n","          output_size=num_dimensions, **graph_network_kwargs)\n","\n","      if self._num_particle_types > 1:\n","        self._particle_type_embedding = tf.get_variable(\n","            \"particle_embedding\",\n","            [self._num_particle_types, particle_type_embedding_size],\n","            trainable=True, use_resource=True)\n","\n","  def _build(self, position_sequence, n_particles_per_example,\n","             global_context=None, particle_types=None):\n","    input_graphs_tuple = self._encoder_preprocessor(\n","        position_sequence, n_particles_per_example, global_context,\n","        particle_types)\n","\n","    normalized_acceleration = self._graph_network(input_graphs_tuple)\n","\n","\n","    next_position = self.get_next_position(normalized_acceleration\n","                                           , position_sequence\n","                                           , self._normalization_stats['acceleration'].mean\n","                                           , self._normalization_stats['acceleration'].std)\n","\n","    return next_position\n","\n","  '''For Node Features'''\n","  # calculate velocity feature for each particles\n","  def get_velocity_features(position_sequence, mean, std):\n","    velocity_sequence = position_sequence[:,1:] - position[:,:-1]\n","    normalized_velocity_sequence = (\n","      velocity_sequence - mean) / std\n","    flat_velocity_sequence = snt.MergeDims(start=1, size=2)(\n","      normalized_velocity_sequence)\n","    return flat_velocity_sequence\n","\n","  # get relationship of each particle with the global boundary\n","  # use a distance to up, bottom, left, right boundaries\n","  # TODO: in future, change the way we evaluate boundary\n","  def get_boundary_features(boundarys, position, norm=1.):\n","    # boundarys [[0.1, 0.9], [0.1, 0.9]]\n","    # 0.1 0.1 lowerbound 0.9, 0.9 upperbound\n","    to_lower_bound = positon - tf.expand_dims(boundarys[:,0], 0) # expand dim maybe because position is in 3d?\n","    to_upper_bound = tf.expand_dims(boundary[:,1], 0) - position\n","    to_boundaries = tf.concat([to_lower_bound, to_upper_bound], axis=1)\n","    return tf.clip_by_value(to_boundaries / norm, -1., 1.)\n","\n","  '''For edge features'''\n","  '''both displacement and distance were mentioned in the paper'''\n","\n","  # calculate displacement for each edge\n","  # TODO: experiment with normalization\n","  def get_relative_displacement_norm(senders, receivers, position, norm=1.):\n","    from_ = tf.gather(position, senders)\n","    to_ = tf.gather(position, receivers)\n","    return (from_ - to_) / norm\n","\n","  # TODO: what if we don't calculate distance\n","  # calculate distance for each edge\n","  def get_relative_distance(displacements)\n","    return tf.norm(displacements, axis=-1, keepdims=True)\n","\n","  def _encoder_preprocessor(\n","      self, position_sequence, n_node, global_context, particle_types):\n","    # Extract important features from the position_sequence.\n","    most_recent_position = position_sequence[:, -1]\n","    velocity_sequence = time_diff(position_sequence)  # Finite-difference.\n","\n","    # Get connectivity of the graph.\n","    (senders, receivers, n_edge\n","     ) = compute_connectivity_for_batch_pyfunc(\n","         most_recent_position, n_node, self._connectivity_radius)\n","\n","    # Collect node features.\n","    node_features = []\n","\n","    node_features.append(get_velocity_features(position_sequence\n","                                               , self._normalization_stats['velocity'].mean\n","                                               , self._normalization_stats['velocity'].std))\n","\n","    node_features.append(get_boundary_features(self._boundaries\n","                                               , most_recent_position\n","                                               , self._connectivity_radius))\n","\n","    # Collect edge features.\n","    edge_features = []\n","\n","    displacements = get_relative_displacement_norm(senders\n","                                                        , receivers\n","                                                        , most_recent_position\n","                                                        , self._connectivity_radius)\n","\n","    distances = get_relative_distance(displacements)\n","    edge_features.append(displacements)\n","    edge_features.append(distances)\n","\n","    # Normalize the global context.\n","    if global_context is not None:\n","      context_stats = self._normalization_stats[\"context\"]\n","      # Context in some datasets are all zero, so add an epsilon for numerical\n","      # stability.\n","      global_context = (global_context - context_stats.mean) / tf.math.maximum(\n","          context_stats.std, STD_EPSILON)\n","\n","    return gn.graphs.GraphsTuple(\n","        nodes=tf.concat(node_features, axis=-1),\n","        edges=tf.concat(edge_features, axis=-1),\n","        globals=global_context,  # self._graph_net will appending this to nodes.\n","        n_node=n_node,\n","        n_edge=n_edge,\n","        senders=senders,\n","        receivers=receivers,\n","        )\n","\n","  # get prediction of the next position using euler integration\n","  def get_next_position(normalized_acceleration, position_sequence, mean, std):\n","    # inverse the normalization done by the network\n","    acceleration = normalized_acceleration * std + mean\n","    previous_velocity = position_sequence[:,-1] - position_sequence[:,-2]\n","    velocity = previous_velocity + acceleration\n","    return position_sequence[:,-1] + velocity\n","\n","\n","  def _decoder_postprocessor(self, normalized_acceleration, position_sequence):\n","\n","    # The model produces the output in normalized space so we apply inverse\n","    # normalization.\n","    acceleration_stats = self._normalization_stats[\"acceleration\"]\n","    acceleration = (\n","        normalized_acceleration * acceleration_stats.std\n","        ) + acceleration_stats.mean\n","\n","    # Use an Euler integrator to go from acceleration to position, assuming\n","    # a dt=1 corresponding to the size of the finite difference.\n","    most_recent_position = position_sequence[:, -1]\n","    most_recent_velocity = most_recent_position - position_sequence[:, -2]\n","\n","    new_velocity = most_recent_velocity + acceleration  # * dt = 1\n","    new_position = most_recent_position + new_velocity  # * dt = 1\n","    return new_position\n","\n","  def get_predicted_and_target_normalized_accelerations(\n","      self, next_position, position_sequence_noise, position_sequence,\n","      n_particles_per_example, global_context=None, particle_types=None):  # pylint: disable=g-doc-args\n","\n","\n","    # Add noise to the input position sequence.\n","    noisy_position_sequence = position_sequence + position_sequence_noise\n","\n","    # Perform the forward pass with the noisy position sequence.\n","    input_graphs_tuple = self._encoder_preprocessor(\n","        noisy_position_sequence, n_particles_per_example, global_context,\n","        particle_types)\n","    predicted_normalized_acceleration = self._graph_network(input_graphs_tuple)\n","\n","    # Calculate the target acceleration, using an `adjusted_next_position `that\n","    # is shifted by the noise in the last input position.\n","    next_position_adjusted = next_position + position_sequence_noise[:, -1]\n","    target_normalized_acceleration = self._inverse_decoder_postprocessor(\n","        next_position_adjusted, noisy_position_sequence)\n","\n","\n","    return predicted_normalized_acceleration, target_normalized_acceleration\n","\n","  def _inverse_decoder_postprocessor(self, next_position, position_sequence):\n","    \"\"\"Inverse of `_decoder_postprocessor`.\"\"\"\n","\n","    previous_position = position_sequence[:, -1]\n","    previous_velocity = previous_position - position_sequence[:, -2]\n","    next_velocity = next_position - previous_position\n","    acceleration = next_velocity - previous_velocity\n","\n","    acceleration_stats = self._normalization_stats[\"acceleration\"]\n","    normalized_acceleration = (\n","        acceleration - acceleration_stats.mean) / acceleration_stats.std\n","    return normalized_acceleration\n","\n","\n","def time_diff(input_sequence):\n","  return input_sequence[:, 1:] - input_sequence[:, :-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olQE4IXyCed4"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"okg2l1Xclfze"},"source":["### helper functions\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3H7gVAysk8WX","executionInfo":{"status":"ok","timestamp":1608008856157,"user_tz":480,"elapsed":1382,"user":{"displayName":"Xiao Liang","photoUrl":"","userId":"15028243839916116675"}},"outputId":"70e57f97-6a7d-4785-ebc1-3ae74ac821b0"},"source":["\n","Stats = collections.namedtuple('Stats', ['mean', 'std'])\n","\n","INPUT_SEQUENCE_LENGTH = 6  # So we can calculate the last 5 velocities.\n","NUM_PARTICLE_TYPES = 9\n","KINEMATIC_PARTICLE_ID = 3\n","\n","\n","def get_kinematic_mask(particle_types):\n","  \"\"\"Returns a boolean mask, set to true for kinematic (obstacle) particles.\"\"\"\n","  return tf.equal(particle_types, KINEMATIC_PARTICLE_ID)\n","\n","\n","def prepare_inputs(tensor_dict):\n","  \"\"\"Prepares a single stack of inputs by calculating inputs and targets.\n","\n","  Computes n_particles_per_example, which is a tensor that contains information\n","  about how to partition the axis - i.e. which nodes belong to which graph.\n","\n","  Adds a batch axis to `n_particles_per_example` and `step_context` so they can\n","  later be batched using `batch_concat`. This batch will be the same as if the\n","  elements had been batched via stacking.\n","\n","  Note that all other tensors have a variable size particle axis,\n","  and in this case they will simply be concatenated along that\n","  axis.\n","\n","\n","\n","  Args:\n","    tensor_dict: A dict of tensors containing positions, and step context (\n","    if available).\n","\n","  Returns:\n","    A tuple of input features and target positions.\n","\n","  \"\"\"\n","  # Position is encoded as [sequence_length, num_particles, dim] but the model\n","  # expects [num_particles, sequence_length, dim].\n","  pos = tensor_dict['position']\n","  pos = tf.transpose(pos, perm=[1, 0, 2])\n","\n","  # The target position is the final step of the stack of positions.\n","  target_position = pos[:, -1]\n","\n","  # Remove the target from the input.\n","  tensor_dict['position'] = pos[:, :-1]\n","\n","  # Compute the number of particles per example.\n","  num_particles = tf.shape(pos)[0]\n","  # Add an extra dimension for stacking via concat.\n","  tensor_dict['n_particles_per_example'] = num_particles[tf.newaxis]\n","\n","  if 'step_context' in tensor_dict:\n","    # Take the input global context. We have a stack of global contexts,\n","    # and we take the penultimate since the final is the target.\n","    tensor_dict['step_context'] = tensor_dict['step_context'][-2]\n","    # Add an extra dimension for stacking via concat.\n","    tensor_dict['step_context'] = tensor_dict['step_context'][tf.newaxis]\n","  return tensor_dict, target_position\n","\n","\n","def prepare_rollout_inputs(context, features):\n","  \"\"\"Prepares an inputs trajectory for rollout.\"\"\"\n","  out_dict = {**context}\n","  # Position is encoded as [sequence_length, num_particles, dim] but the model\n","  # expects [num_particles, sequence_length, dim].\n","  pos = tf.transpose(features['position'], [1, 0, 2])\n","  # The target position is the final step of the stack of positions.\n","  target_position = pos[:, -1]\n","  # Remove the target from the input.\n","  out_dict['position'] = pos[:, :-1]\n","  # Compute the number of nodes\n","  out_dict['n_particles_per_example'] = [tf.shape(pos)[0]]\n","  if 'step_context' in features:\n","    out_dict['step_context'] = features['step_context']\n","  out_dict['is_trajectory'] = tf.constant([True], tf.bool)\n","  return out_dict, target_position\n","\n","\n","def batch_concat(dataset, batch_size):\n","  \"\"\"We implement batching as concatenating on the leading axis.\"\"\"\n","\n","  # We create a dataset of datasets of length batch_size.\n","  windowed_ds = dataset.window(batch_size)\n","\n","  # The plan is then to reduce every nested dataset by concatenating. We can\n","  # do this using tf.data.Dataset.reduce. This requires an initial state, and\n","  # then incrementally reduces by running through the dataset\n","\n","  # Get initial state. In this case this will be empty tensors of the\n","  # correct shape.\n","  initial_state = tree.map_structure(\n","      lambda spec: tf.zeros(  # pylint: disable=g-long-lambda\n","          shape=[0] + spec.shape.as_list()[1:], dtype=spec.dtype),\n","      dataset.element_spec)\n","\n","  # We run through the nest and concatenate each entry with the previous state.\n","  def reduce_window(initial_state, ds):\n","    return ds.reduce(initial_state, lambda x, y: tf.concat([x, y], axis=0))\n","\n","  return windowed_ds.map(\n","      lambda *x: tree.map_structure(reduce_window, initial_state, x))\n","\n","\n","def get_input_fn(data_path, batch_size, mode, split):\n","  \"\"\"Gets the learning simulation input function for tf.estimator.Estimator.\n","\n","  Args:\n","    data_path: the path to the dataset directory.\n","    batch_size: the number of graphs in a batch.\n","    mode: either 'one_step_train', 'one_step' or 'rollout'\n","    split: either 'train', 'valid' or 'test.\n","\n","  Returns:\n","    The input function for the learning simulation model.\n","  \"\"\"\n","  def input_fn():\n","    \"\"\"Input function for learning simulation.\"\"\"\n","    # Loads the metadata of the dataset.\n","    metadata = _read_metadata(data_path)\n","    # Create a tf.data.Dataset from the TFRecord.\n","    ds = tf.data.TFRecordDataset([os.path.join(data_path, f'{split}.tfrecord')])\n","    ds = ds.map(functools.partial(parse_serialized_simulation_example, metadata=metadata))\n","    if mode.startswith('one_step'):\n","      # Splits an entire trajectory into chunks of 7 steps.\n","      # Previous 5 velocities, current velocity and target.\n","      split_with_window = functools.partial(\n","          split_trajectory,\n","          window_length=INPUT_SEQUENCE_LENGTH + 1)\n","      ds = ds.flat_map(split_with_window)\n","      # Splits a chunk into input steps and target steps\n","      ds = ds.map(prepare_inputs)\n","      # If in train mode, repeat dataset forever and shuffle.\n","      if mode == 'one_step_train':\n","        ds = ds.repeat()\n","        ds = ds.shuffle(512)\n","      # Custom batching on the leading axis.\n","      ds = batch_concat(ds, batch_size)\n","    elif mode == 'rollout':\n","      # Rollout evaluation only available for batch size 1\n","      assert batch_size == 1\n","      ds = ds.map(prepare_rollout_inputs)\n","    else:\n","      raise ValueError(f'mode: {mode} not recognized')\n","    return ds\n","\n","  return input_fn\n","\n","\n","def rollout(simulator, features, num_steps):\n","  \"\"\"Rolls out a trajectory by applying the model in sequence.\"\"\"\n","  initial_positions = features['position'][:, 0:INPUT_SEQUENCE_LENGTH]\n","  ground_truth_positions = features['position'][:, INPUT_SEQUENCE_LENGTH:]\n","  global_context = features.get('step_context')\n","  def step_fn(step, current_positions, predictions):\n","\n","    if global_context is None:\n","      global_context_step = None\n","    else:\n","      global_context_step = global_context[\n","          step + INPUT_SEQUENCE_LENGTH - 1][tf.newaxis]\n","\n","    next_position = simulator(\n","        current_positions,\n","        n_particles_per_example=features['n_particles_per_example'],\n","        particle_types=features['particle_type'],\n","        global_context=global_context_step)\n","\n","    # Update kinematic particles from prescribed trajectory.\n","    kinematic_mask = get_kinematic_mask(features['particle_type'])\n","    next_position_ground_truth = ground_truth_positions[:, step]\n","    next_position = tf.where(kinematic_mask, next_position_ground_truth,\n","                             next_position)\n","    updated_predictions = predictions.write(step, next_position)\n","\n","    # Shift `current_positions`, removing the oldest position in the sequence\n","    # and appending the next position at the end.\n","    next_positions = tf.concat([current_positions[:, 1:],\n","                                next_position[:, tf.newaxis]], axis=1)\n","\n","    return (step + 1, next_positions, updated_predictions)\n","\n","  predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n","  _, _, predictions = tf.while_loop(\n","      cond=lambda step, state, prediction: tf.less(step, num_steps),\n","      body=step_fn,\n","      loop_vars=(0, initial_positions, predictions),\n","      back_prop=False,\n","      parallel_iterations=1)\n","\n","  output_dict = {\n","      'initial_positions': tf.transpose(initial_positions, [1, 0, 2]),\n","      'predicted_rollout': predictions.stack(),\n","      'ground_truth_rollout': tf.transpose(ground_truth_positions, [1, 0, 2]),\n","      'particle_types': features['particle_type'],\n","  }\n","\n","  if global_context is not None:\n","    output_dict['global_context'] = global_context\n","  return output_dict\n","\n","\n","def _combine_std(std_x, std_y):\n","  return np.sqrt(std_x**2 + std_y**2)\n","\n","\n","def _get_simulator(model_kwargs, metadata, acc_noise_std, vel_noise_std):\n","  \"\"\"Instantiates the simulator.\"\"\"\n","  # Cast statistics to numpy so they are arrays when entering the model.\n","  cast = lambda v: np.array(v, dtype=np.float32)\n","  acceleration_stats = Stats(\n","      cast(metadata['acc_mean']),\n","      _combine_std(cast(metadata['acc_std']), acc_noise_std))\n","  velocity_stats = Stats(\n","      cast(metadata['vel_mean']),\n","      _combine_std(cast(metadata['vel_std']), vel_noise_std))\n","  normalization_stats = {'acceleration': acceleration_stats,\n","                         'velocity': velocity_stats}\n","  if 'context_mean' in metadata:\n","    context_stats = Stats(\n","        cast(metadata['context_mean']), cast(metadata['context_std']))\n","    normalization_stats['context'] = context_stats\n","\n","  simulator = LearnedSimulator(\n","      num_dimensions=metadata['dim'],\n","      connectivity_radius=metadata['default_connectivity_radius'],\n","      graph_network_kwargs=model_kwargs,\n","      boundaries=metadata['bounds'],\n","      num_particle_types=NUM_PARTICLE_TYPES,\n","      normalization_stats=normalization_stats,\n","      particle_type_embedding_size=16)\n","  return simulator\n","\n","\n","def get_one_step_estimator_fn(data_path,\n","                              noise_std,\n","                              latent_size=128,\n","                              hidden_size=128,\n","                              hidden_layers=2,\n","                              message_passing_steps=10):\n","  \"\"\"Gets one step model for training simulation.\"\"\"\n","  metadata = _read_metadata(data_path)\n","\n","  model_kwargs = dict(\n","      latent_size=latent_size,\n","      mlp_hidden_size=hidden_size,\n","      mlp_num_hidden_layers=hidden_layers,\n","      num_message_passing_steps=message_passing_steps)\n","\n","  def estimator_fn(features, labels, mode):\n","    target_next_position = labels\n","    simulator = _get_simulator(model_kwargs, metadata,\n","                               vel_noise_std=noise_std,\n","                               acc_noise_std=noise_std)\n","    # Sample the noise to add to the inputs to the model during training.\n","    sampled_noise = get_random_walk_noise_for_position_sequence(\n","        features['position'], noise_std_last_step=noise_std)\n","    non_kinematic_mask = tf.logical_not(\n","        get_kinematic_mask(features['particle_type']))\n","    noise_mask = tf.cast(\n","        non_kinematic_mask, sampled_noise.dtype)[:, tf.newaxis, tf.newaxis]\n","    sampled_noise *= noise_mask\n","\n","    # Get the predictions and target accelerations.\n","    pred_target = simulator.get_predicted_and_target_normalized_accelerations(\n","        next_position=target_next_position,\n","        position_sequence=features['position'],\n","        position_sequence_noise=sampled_noise,\n","        n_particles_per_example=features['n_particles_per_example'],\n","        particle_types=features['particle_type'],\n","        global_context=features.get('step_context'))\n","    pred_acceleration, target_acceleration = pred_target\n","\n","    # Calculate the loss and mask out loss on kinematic particles/\n","    loss = (pred_acceleration - target_acceleration)**2\n","\n","    num_non_kinematic = tf.reduce_sum(\n","        tf.cast(non_kinematic_mask, tf.float32))\n","    loss = tf.where(non_kinematic_mask, loss, tf.zeros_like(loss))\n","    loss = tf.reduce_sum(loss) / tf.reduce_sum(num_non_kinematic)\n","    global_step = tf.train.get_global_step()\n","    # Set learning rate to decay from 1e-4 to 1e-6 exponentially.\n","    min_lr = 1e-6\n","    lr = tf.train.exponential_decay(learning_rate=1e-4 - min_lr,\n","                                    global_step=global_step,\n","                                    decay_steps=int(5e6),\n","                                    decay_rate=0.1) + min_lr\n","    opt = tf.train.AdamOptimizer(learning_rate=lr)\n","    train_op = opt.minimize(loss, global_step)\n","\n","    # Calculate next position and add some additional eval metrics (only eval).\n","    predicted_next_position = simulator(\n","        position_sequence=features['position'],\n","        n_particles_per_example=features['n_particles_per_example'],\n","        particle_types=features['particle_type'],\n","        global_context=features.get('step_context'))\n","\n","    predictions = {'predicted_next_position': predicted_next_position}\n","\n","    eval_metrics_ops = {\n","        'loss_mse': tf.metrics.mean_squared_error(\n","            pred_acceleration, target_acceleration),\n","        'one_step_position_mse': tf.metrics.mean_squared_error(\n","            predicted_next_position, target_next_position)\n","    }\n","    return tf.estimator.EstimatorSpec(\n","        mode=mode,\n","        train_op=train_op,\n","        loss=loss,\n","        predictions=predictions,\n","        eval_metric_ops=eval_metrics_ops)\n","\n","  return estimator_fn\n","\n","\n","def get_rollout_estimator_fn(data_path,\n","                             noise_std,\n","                             latent_size=128,\n","                             hidden_size=128,\n","                             hidden_layers=2,\n","                             message_passing_steps=10):\n","  \"\"\"Gets the model function for tf.estimator.Estimator.\"\"\"\n","  metadata = _read_metadata(data_path)\n","\n","  model_kwargs = dict(\n","      latent_size=latent_size,\n","      mlp_hidden_size=hidden_size,\n","      mlp_num_hidden_layers=hidden_layers,\n","      num_message_passing_steps=message_passing_steps)\n","\n","  def estimator_fn(features, labels, mode):\n","    del labels  # Labels to conform to estimator spec.\n","    simulator = _get_simulator(model_kwargs, metadata,\n","                               acc_noise_std=noise_std,\n","                               vel_noise_std=noise_std)\n","\n","    num_steps = metadata['sequence_length'] - INPUT_SEQUENCE_LENGTH\n","    rollout_op = rollout(simulator, features, num_steps=num_steps)\n","    squared_error = (rollout_op['predicted_rollout'] -\n","                     rollout_op['ground_truth_rollout']) ** 2\n","    loss = tf.reduce_mean(squared_error)\n","    eval_ops = {'rollout_error_mse': tf.metrics.mean_squared_error(\n","        rollout_op['predicted_rollout'], rollout_op['ground_truth_rollout'])}\n","\n","    # Add a leading axis, since Estimator's predict method insists that all\n","    # tensors have a shared leading batch axis fo the same dims.\n","    rollout_op = tree.map_structure(lambda x: x[tf.newaxis], rollout_op)\n","    return tf.estimator.EstimatorSpec(\n","        mode=mode,\n","        train_op=None,\n","        loss=loss,\n","        predictions=rollout_op,\n","        eval_metric_ops=eval_ops)\n","\n","  return estimator_fn\n","\n","\n","def _read_metadata(data_path):\n","  with open(os.path.join(data_path, 'metadata.json'), 'rt') as fp:\n","    return json.loads(fp.read())\n","\n","'''\n","def main(_):\n","  \"\"\"Train or evaluates the model.\"\"\"\n","\n","  if FLAGS.mode in ['train', 'eval']:\n","    estimator = tf.estimator.Estimator(\n","        get_one_step_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\n","        model_dir=FLAGS.model_path)\n","    if FLAGS.mode == 'train':\n","      # Train all the way through.\n","      estimator.train(\n","          input_fn=get_input_fn(FLAGS.data_path, FLAGS.batch_size,\n","                                mode='one_step_train', split='train'),\n","          max_steps=FLAGS.num_steps)\n","    else:\n","      # One-step evaluation from checkpoint.\n","      eval_metrics = estimator.evaluate(input_fn=get_input_fn(\n","          FLAGS.data_path, FLAGS.batch_size,\n","          mode='one_step', split=FLAGS.eval_split))\n","      logging.info('Evaluation metrics:')\n","      logging.info(eval_metrics)\n","  elif FLAGS.mode == 'eval_rollout':\n","    if not FLAGS.output_path:\n","      raise ValueError('A rollout path must be provided.')\n","    rollout_estimator = tf.estimator.Estimator(\n","        get_rollout_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\n","        model_dir=FLAGS.model_path)\n","\n","    # Iterate through rollouts saving them one by one.\n","    metadata = _read_metadata(FLAGS.data_path)\n","    rollout_iterator = rollout_estimator.predict(\n","        input_fn=get_input_fn(FLAGS.data_path, batch_size=1,\n","                              mode='rollout', split=FLAGS.eval_split))\n","\n","    for example_index, example_rollout in enumerate(rollout_iterator):\n","      example_rollout['metadata'] = metadata\n","      filename = f'rollout_{FLAGS.eval_split}_{example_index}.pkl'\n","      filename = os.path.join(FLAGS.output_path, filename)\n","      logging.info('Saving: %s.', filename)\n","      if not os.path.exists(FLAGS.output_path):\n","        os.mkdir(FLAGS.output_path)\n","      with open(filename, 'wb') as file:\n","        pickle.dump(example_rollout, file)\n","\n","if __name__ == '__main__':\n","  tf.disable_v2_behavior()\n","  app.run(main)\n","'''\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ndef main(_):\\n  \"\"\"Train or evaluates the model.\"\"\"\\n\\n  if FLAGS.mode in [\\'train\\', \\'eval\\']:\\n    estimator = tf.estimator.Estimator(\\n        get_one_step_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\\n        model_dir=FLAGS.model_path)\\n    if FLAGS.mode == \\'train\\':\\n      # Train all the way through.\\n      estimator.train(\\n          input_fn=get_input_fn(FLAGS.data_path, FLAGS.batch_size,\\n                                mode=\\'one_step_train\\', split=\\'train\\'),\\n          max_steps=FLAGS.num_steps)\\n    else:\\n      # One-step evaluation from checkpoint.\\n      eval_metrics = estimator.evaluate(input_fn=get_input_fn(\\n          FLAGS.data_path, FLAGS.batch_size,\\n          mode=\\'one_step\\', split=FLAGS.eval_split))\\n      logging.info(\\'Evaluation metrics:\\')\\n      logging.info(eval_metrics)\\n  elif FLAGS.mode == \\'eval_rollout\\':\\n    if not FLAGS.output_path:\\n      raise ValueError(\\'A rollout path must be provided.\\')\\n    rollout_estimator = tf.estimator.Estimator(\\n        get_rollout_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\\n        model_dir=FLAGS.model_path)\\n\\n    # Iterate through rollouts saving them one by one.\\n    metadata = _read_metadata(FLAGS.data_path)\\n    rollout_iterator = rollout_estimator.predict(\\n        input_fn=get_input_fn(FLAGS.data_path, batch_size=1,\\n                              mode=\\'rollout\\', split=FLAGS.eval_split))\\n\\n    for example_index, example_rollout in enumerate(rollout_iterator):\\n      example_rollout[\\'metadata\\'] = metadata\\n      filename = f\\'rollout_{FLAGS.eval_split}_{example_index}.pkl\\'\\n      filename = os.path.join(FLAGS.output_path, filename)\\n      logging.info(\\'Saving: %s.\\', filename)\\n      if not os.path.exists(FLAGS.output_path):\\n        os.mkdir(FLAGS.output_path)\\n      with open(filename, \\'wb\\') as file:\\n        pickle.dump(example_rollout, file)\\n\\nif __name__ == \\'__main__\\':\\n  tf.disable_v2_behavior()\\n  app.run(main)\\n'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"iSURALrClnmt"},"source":["### main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQGbvs1qlrnw","outputId":"f67b0c27-c993-471e-dfa4-632b27d41166"},"source":["PROJECT_PATH = '/gdrive/MyDrive/CSE 490G/Final Project/'\n","data_path = PROJECT_PATH + 'data'\n","model_path = PROJECT_PATH + 'model'\n","num_steps = int(2e4)\n","noise_std = 6.7e-4\n","batch_size = 2\n","\n","\n","\n","estimator = tf.estimator.Estimator(\n","        get_one_step_estimator_fn(data_path, noise_std),\n","        model_dir=model_path)\n","estimator.train(\n","          input_fn=get_input_fn(data_path, batch_size,\n","                                mode='one_step_train', split='train'),\n","          max_steps=num_steps)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","INFO:tensorflow:Using config: {'_model_dir': '/gdrive/MyDrive/CSE 490G/Final Project/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0974692438>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:Entity <function _yield_value at 0x7f094dde92f0> appears to be a generator function. It will not be converted by AutoGraph.\n","WARNING: Entity <function _yield_value at 0x7f094dde92f0> appears to be a generator function. It will not be converted by AutoGraph.\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/basic.py:127: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/basic.py:132: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From <ipython-input-9-80f85109fb84>:280: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt-10000\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 10000 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:loss = 0.26849398, step = 10001\n","INFO:tensorflow:global_step/sec: 0.265367\n","INFO:tensorflow:loss = 0.67046076, step = 10101 (376.839 sec)\n","INFO:tensorflow:Saving checkpoints for 10164 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","INFO:tensorflow:global_step/sec: 0.295471\n","INFO:tensorflow:loss = 0.1315368, step = 10201 (338.446 sec)\n","INFO:tensorflow:global_step/sec: 0.307781\n","INFO:tensorflow:loss = 0.16422494, step = 10301 (324.903 sec)\n","INFO:tensorflow:Saving checkpoints for 10351 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.316198\n","INFO:tensorflow:loss = 0.0954009, step = 10401 (316.257 sec)\n","INFO:tensorflow:global_step/sec: 0.303321\n","INFO:tensorflow:loss = 0.7545498, step = 10501 (329.684 sec)\n","INFO:tensorflow:Saving checkpoints for 10533 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.268128\n","INFO:tensorflow:loss = 0.16611008, step = 10601 (372.956 sec)\n","INFO:tensorflow:Saving checkpoints for 10684 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.242855\n","INFO:tensorflow:loss = 0.118207045, step = 10701 (411.768 sec)\n","INFO:tensorflow:global_step/sec: 0.237131\n","INFO:tensorflow:loss = 0.7195744, step = 10801 (421.711 sec)\n","INFO:tensorflow:Saving checkpoints for 10826 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.214071\n","INFO:tensorflow:loss = 0.18494551, step = 10901 (467.131 sec)\n","INFO:tensorflow:Saving checkpoints for 10951 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.204152\n","INFO:tensorflow:loss = 0.1107573, step = 11001 (489.830 sec)\n","INFO:tensorflow:Saving checkpoints for 11082 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.225139\n","INFO:tensorflow:loss = 0.13072695, step = 11101 (444.169 sec)\n","INFO:tensorflow:global_step/sec: 0.246593\n","INFO:tensorflow:loss = 0.151148, step = 11201 (405.529 sec)\n","INFO:tensorflow:Saving checkpoints for 11228 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.24901\n","INFO:tensorflow:loss = 0.22917049, step = 11301 (401.590 sec)\n","INFO:tensorflow:Saving checkpoints for 11387 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.275221\n","INFO:tensorflow:loss = 0.27273038, step = 11401 (363.343 sec)\n","INFO:tensorflow:global_step/sec: 0.297668\n","INFO:tensorflow:loss = 0.17755735, step = 11501 (335.945 sec)\n","INFO:tensorflow:Saving checkpoints for 11563 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.289453\n","INFO:tensorflow:loss = 0.18997163, step = 11601 (345.479 sec)\n","INFO:tensorflow:global_step/sec: 0.28743\n","INFO:tensorflow:loss = 0.13281222, step = 11701 (347.912 sec)\n","INFO:tensorflow:Saving checkpoints for 11735 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.267572\n","INFO:tensorflow:loss = 0.08248133, step = 11801 (373.734 sec)\n","INFO:tensorflow:Saving checkpoints for 11889 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.248171\n","INFO:tensorflow:loss = 0.13870573, step = 11901 (402.945 sec)\n","INFO:tensorflow:global_step/sec: 0.24288\n","INFO:tensorflow:loss = 0.17870486, step = 12001 (411.726 sec)\n","INFO:tensorflow:Saving checkpoints for 12035 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.245526\n","INFO:tensorflow:loss = 0.1423808, step = 12101 (407.288 sec)\n","INFO:tensorflow:Saving checkpoints for 12186 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.255049\n","INFO:tensorflow:loss = 0.10485015, step = 12201 (392.083 sec)\n","INFO:tensorflow:global_step/sec: 0.284313\n","INFO:tensorflow:loss = 0.18566583, step = 12301 (351.724 sec)\n","INFO:tensorflow:Saving checkpoints for 12360 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.305811\n","INFO:tensorflow:loss = 0.2071133, step = 12401 (327.004 sec)\n","INFO:tensorflow:global_step/sec: 0.291059\n","INFO:tensorflow:loss = 0.43935493, step = 12501 (343.569 sec)\n","INFO:tensorflow:Saving checkpoints for 12540 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.312877\n","INFO:tensorflow:loss = 0.5011006, step = 12601 (319.617 sec)\n","INFO:tensorflow:global_step/sec: 0.293841\n","INFO:tensorflow:loss = 0.32131213, step = 12701 (340.317 sec)\n","INFO:tensorflow:Saving checkpoints for 12719 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.265196\n","INFO:tensorflow:loss = 0.2873589, step = 12801 (377.080 sec)\n","INFO:tensorflow:Saving checkpoints for 12871 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.242232\n","INFO:tensorflow:loss = 0.10569476, step = 12901 (412.828 sec)\n","INFO:tensorflow:global_step/sec: 0.240661\n","INFO:tensorflow:loss = 0.088544786, step = 13001 (415.521 sec)\n","INFO:tensorflow:Saving checkpoints for 13016 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.238057\n","INFO:tensorflow:loss = 0.21031058, step = 13101 (420.068 sec)\n","INFO:tensorflow:Saving checkpoints for 13158 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.227228\n","INFO:tensorflow:loss = 0.1499533, step = 13201 (440.087 sec)\n","INFO:tensorflow:Saving checkpoints for 13290 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.214104\n","INFO:tensorflow:loss = 0.12132894, step = 13301 (467.061 sec)\n","INFO:tensorflow:global_step/sec: 0.220012\n","INFO:tensorflow:loss = 0.1915355, step = 13401 (454.520 sec)\n","INFO:tensorflow:Saving checkpoints for 13425 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.263703\n","INFO:tensorflow:loss = 0.12497997, step = 13501 (379.215 sec)\n","INFO:tensorflow:Saving checkpoints for 13586 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.268261\n","INFO:tensorflow:loss = 0.13030088, step = 13601 (372.774 sec)\n","INFO:tensorflow:global_step/sec: 0.292197\n","INFO:tensorflow:loss = 0.37828663, step = 13701 (342.233 sec)\n","INFO:tensorflow:Saving checkpoints for 13754 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.258228\n","INFO:tensorflow:loss = 0.31890935, step = 13801 (387.254 sec)\n","INFO:tensorflow:Saving checkpoints for 13901 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.23841\n","INFO:tensorflow:loss = 0.08159376, step = 13901 (419.446 sec)\n","INFO:tensorflow:global_step/sec: 0.233758\n","INFO:tensorflow:loss = 0.15220585, step = 14001 (427.791 sec)\n","INFO:tensorflow:Saving checkpoints for 14045 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.246873\n","INFO:tensorflow:loss = 0.10608971, step = 14101 (405.068 sec)\n","INFO:tensorflow:Saving checkpoints for 14198 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.259711\n","INFO:tensorflow:loss = 0.110357806, step = 14201 (385.043 sec)\n","INFO:tensorflow:global_step/sec: 0.272832\n","INFO:tensorflow:loss = 0.089105494, step = 14301 (366.526 sec)\n","INFO:tensorflow:Saving checkpoints for 14370 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.308896\n","INFO:tensorflow:loss = 0.09349061, step = 14401 (323.732 sec)\n","INFO:tensorflow:global_step/sec: 0.327129\n","INFO:tensorflow:loss = 0.13602108, step = 14501 (305.691 sec)\n","INFO:tensorflow:Saving checkpoints for 14565 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.322837\n","INFO:tensorflow:loss = 0.28390515, step = 14601 (309.756 sec)\n","INFO:tensorflow:global_step/sec: 0.312819\n","INFO:tensorflow:loss = 0.090308085, step = 14701 (319.672 sec)\n","INFO:tensorflow:Saving checkpoints for 14752 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.291984\n","INFO:tensorflow:loss = 0.15038463, step = 14801 (342.484 sec)\n","INFO:tensorflow:global_step/sec: 0.278351\n","INFO:tensorflow:loss = 0.12779395, step = 14901 (359.260 sec)\n","INFO:tensorflow:Saving checkpoints for 14922 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.27189\n","INFO:tensorflow:loss = 0.07837609, step = 15001 (367.794 sec)\n","INFO:tensorflow:Saving checkpoints for 15079 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.249564\n","INFO:tensorflow:loss = 0.068568, step = 15101 (400.701 sec)\n","INFO:tensorflow:global_step/sec: 0.237458\n","INFO:tensorflow:loss = 0.049377788, step = 15201 (421.125 sec)\n","INFO:tensorflow:Saving checkpoints for 15222 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.235929\n","INFO:tensorflow:loss = 0.13320084, step = 15301 (423.857 sec)\n","INFO:tensorflow:Saving checkpoints for 15359 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.211961\n","INFO:tensorflow:loss = 0.071107715, step = 15401 (471.784 sec)\n","INFO:tensorflow:Saving checkpoints for 15488 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.217269\n","INFO:tensorflow:loss = 0.08325151, step = 15501 (460.258 sec)\n","INFO:tensorflow:global_step/sec: 0.255497\n","INFO:tensorflow:loss = 0.1743165, step = 15601 (391.397 sec)\n","INFO:tensorflow:Saving checkpoints for 15644 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.275729\n","INFO:tensorflow:loss = 0.091977715, step = 15701 (362.674 sec)\n","INFO:tensorflow:global_step/sec: 0.282016\n","INFO:tensorflow:loss = 0.9324234, step = 15801 (354.588 sec)\n","INFO:tensorflow:Saving checkpoints for 15812 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.2749\n","INFO:tensorflow:loss = 0.09934113, step = 15901 (363.768 sec)\n","INFO:tensorflow:Saving checkpoints for 15972 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.256926\n","INFO:tensorflow:loss = 0.23881893, step = 16001 (389.217 sec)\n","INFO:tensorflow:global_step/sec: 0.275149\n","INFO:tensorflow:loss = 0.15526132, step = 16101 (363.441 sec)\n","INFO:tensorflow:Saving checkpoints for 16139 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.298066\n","INFO:tensorflow:loss = 0.58468705, step = 16201 (335.496 sec)\n","INFO:tensorflow:global_step/sec: 0.307956\n","INFO:tensorflow:loss = 0.059487864, step = 16301 (324.720 sec)\n","INFO:tensorflow:Saving checkpoints for 16324 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.324909\n","INFO:tensorflow:loss = 0.29745877, step = 16401 (307.779 sec)\n","INFO:tensorflow:global_step/sec: 0.330946\n","INFO:tensorflow:loss = 0.13362575, step = 16501 (302.164 sec)\n","INFO:tensorflow:Saving checkpoints for 16520 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.317305\n","INFO:tensorflow:loss = 0.2629783, step = 16601 (315.154 sec)\n","INFO:tensorflow:global_step/sec: 0.291079\n","INFO:tensorflow:loss = 0.1797415, step = 16701 (343.549 sec)\n","INFO:tensorflow:Saving checkpoints for 16702 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.253249\n","INFO:tensorflow:loss = 0.1276234, step = 16801 (394.869 sec)\n","INFO:tensorflow:Saving checkpoints for 16848 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.220238\n","INFO:tensorflow:loss = 0.11668459, step = 16901 (454.054 sec)\n","INFO:tensorflow:Saving checkpoints for 16978 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.211854\n","INFO:tensorflow:loss = 0.13280915, step = 17001 (472.024 sec)\n","INFO:tensorflow:global_step/sec: 0.210472\n","INFO:tensorflow:loss = 0.1334907, step = 17101 (475.124 sec)\n","INFO:tensorflow:Saving checkpoints for 17104 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.202132\n","INFO:tensorflow:loss = 0.091051035, step = 17201 (494.730 sec)\n","INFO:tensorflow:Saving checkpoints for 17227 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.20875\n","INFO:tensorflow:loss = 0.07929757, step = 17301 (479.039 sec)\n","INFO:tensorflow:Saving checkpoints for 17352 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.206771\n","INFO:tensorflow:loss = 0.13901097, step = 17401 (483.627 sec)\n","INFO:tensorflow:Saving checkpoints for 17475 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.198193\n","INFO:tensorflow:loss = 0.07997841, step = 17501 (504.558 sec)\n","INFO:tensorflow:Saving checkpoints for 17600 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.214186\n","INFO:tensorflow:loss = 0.12089343, step = 17601 (466.884 sec)\n","INFO:tensorflow:global_step/sec: 0.243422\n","INFO:tensorflow:loss = 0.12989786, step = 17701 (410.809 sec)\n","INFO:tensorflow:Saving checkpoints for 17747 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.248158\n","INFO:tensorflow:loss = 0.28800693, step = 17801 (402.969 sec)\n","INFO:tensorflow:Saving checkpoints for 17896 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.244874\n","INFO:tensorflow:loss = 0.5070723, step = 17901 (408.373 sec)\n","INFO:tensorflow:global_step/sec: 0.230347\n","INFO:tensorflow:loss = 0.083713844, step = 18001 (434.127 sec)\n","INFO:tensorflow:Saving checkpoints for 18032 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.205241\n","INFO:tensorflow:loss = 0.099542305, step = 18101 (487.234 sec)\n","INFO:tensorflow:Saving checkpoints for 18159 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.224896\n","INFO:tensorflow:loss = 0.12692457, step = 18201 (444.649 sec)\n","INFO:tensorflow:global_step/sec: 0.24753\n","INFO:tensorflow:loss = 0.31056678, step = 18301 (403.992 sec)\n","INFO:tensorflow:Saving checkpoints for 18305 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.266109\n","INFO:tensorflow:loss = 0.070958935, step = 18401 (375.785 sec)\n","INFO:tensorflow:Saving checkpoints for 18465 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.25979\n","INFO:tensorflow:loss = 0.077839546, step = 18501 (384.928 sec)\n","INFO:tensorflow:global_step/sec: 0.246419\n","INFO:tensorflow:loss = 0.21679161, step = 18601 (405.811 sec)\n","INFO:tensorflow:Saving checkpoints for 18612 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.215497\n","INFO:tensorflow:loss = 0.11108788, step = 18701 (464.043 sec)\n","INFO:tensorflow:Saving checkpoints for 18744 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.236099\n","INFO:tensorflow:loss = 0.17339148, step = 18801 (423.552 sec)\n","INFO:tensorflow:Saving checkpoints for 18888 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.239856\n","INFO:tensorflow:loss = 0.10128496, step = 18901 (416.915 sec)\n","INFO:tensorflow:global_step/sec: 0.248339\n","INFO:tensorflow:loss = 0.05731855, step = 19001 (402.678 sec)\n","INFO:tensorflow:Saving checkpoints for 19039 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.259661\n","INFO:tensorflow:loss = 0.146928, step = 19101 (385.115 sec)\n","INFO:tensorflow:Saving checkpoints for 19189 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.238472\n","INFO:tensorflow:loss = 0.06358998, step = 19201 (419.335 sec)\n","INFO:tensorflow:global_step/sec: 0.196269\n","INFO:tensorflow:loss = 0.17231862, step = 19301 (509.508 sec)\n","INFO:tensorflow:Saving checkpoints for 19309 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.198643\n","INFO:tensorflow:loss = 0.21177511, step = 19401 (503.418 sec)\n","INFO:tensorflow:Saving checkpoints for 19429 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.194847\n","INFO:tensorflow:loss = 0.12096601, step = 19501 (513.218 sec)\n","INFO:tensorflow:Saving checkpoints for 19543 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.181905\n","INFO:tensorflow:loss = 0.054900065, step = 19601 (549.737 sec)\n","INFO:tensorflow:Saving checkpoints for 19657 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.20392\n","INFO:tensorflow:loss = 0.20397146, step = 19701 (490.392 sec)\n","INFO:tensorflow:Saving checkpoints for 19786 into /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.213049\n","INFO:tensorflow:loss = 0.054654043, step = 19801 (469.370 sec)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DbCJodEygp50"},"source":["# Test and Eval"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QgcrfMgxg-SV","outputId":"12f82d06-ddd4-4fe5-d041-666a426cb900"},"source":["PROJECT_PATH = '/gdrive/MyDrive/CSE 490G/Final Project/'\n","rollout_path = PROJECT_PATH + 'rollouts'\n","data_path = PROJECT_PATH + 'data'\n","model_path = PROJECT_PATH + 'model'\n","noise_std = 6.7e-4\n","\n","rollout_estimator = tf.estimator.Estimator(\n","    get_rollout_estimator_fn(data_path, noise_std)\n","    ,model_dir=model_path)\n","\n","# Iterate through rollouts saving them one by one.\n","metadata = _read_metadata(data_path)\n","rollout_iterator = rollout_estimator.predict(input_fn=get_input_fn(data_path\n","                                                                   , batch_size=1\n","                                                                   ,mode='rollout'\n","                                                                   , split='test'))\n","\n","for example_index, example_rollout in enumerate(rollout_iterator):\n","  print(example_index)\n","  example_rollout['metadata'] = metadata\n","  filename = f'rollout_test_{example_index}.pkl'\n","  filename = os.path.join(rollout_path, filename)\n","  logging.info('Saving: %s.', filename)\n","  if not os.path.exists(rollout_path):\n","    os.mkdir(rollout_path)\n","  with open(filename, 'wb') as file:\n","    pickle.dump(example_rollout, file)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","INFO:tensorflow:Using config: {'_model_dir': '/gdrive/MyDrive/CSE 490G/Final Project/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa258871470>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/basic.py:127: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/basic.py:132: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From <ipython-input-8-80f85109fb84>:174: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt-19786\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OM23S0UslEwn"},"source":["### render trajectories"]}]}