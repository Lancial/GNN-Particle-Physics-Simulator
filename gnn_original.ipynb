{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GNN_v4.ipynb","provenance":[],"authorship_tag":"ABX9TyMj8rHMbTTeN6x2pLeZsBce"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m7I2PDGdguQw"},"source":["# Setup\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B243Ji0Vz8zr","executionInfo":{"status":"ok","timestamp":1607971557453,"user_tz":480,"elapsed":1126,"user":{"displayName":"Xiao Liang","photoUrl":"","userId":"15028243839916116675"}},"outputId":"d0a0cf4d-7268-4d07-8516-acb0908cdea3"},"source":["from google.colab import drive\n","drive.mount('/gdrive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kkiii4LI0GFa"},"source":["!bash pip install -r /gdrive/MyDrive/CSE\\ 490G/Final\\ Project/requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2cu9_Tu0FHo"},"source":["import graph_nets as gn\n","import sonnet as snt\n","import functools\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","import collections\n","import json\n","import os\n","import pickle\n","import tree\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","from absl import app\n","from absl import flags\n","from absl import logging\n","from sklearn import neighbors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aoJPRZRrCmQO"},"source":["# Utils (connectivity, data loading, noise)"]},{"cell_type":"markdown","metadata":{"id":"s2trO4wYj4yH"},"source":["### data loading"]},{"cell_type":"code","metadata":{"id":"8DdAIbmdjmRK"},"source":["# Create a description of the features.\n","_FEATURE_DESCRIPTION = {\n","    'position': tf.io.VarLenFeature(tf.string),\n","}\n","\n","_FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT = _FEATURE_DESCRIPTION.copy()\n","_FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT['step_context'] = tf.io.VarLenFeature(\n","    tf.string)\n","\n","_FEATURE_DTYPES = {\n","    'position': {\n","        'in': np.float32,\n","        'out': tf.float32\n","    },\n","    'step_context': {\n","        'in': np.float32,\n","        'out': tf.float32\n","    }\n","}\n","\n","_CONTEXT_FEATURES = {\n","    'key': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n","    'particle_type': tf.io.VarLenFeature(tf.string)\n","}\n","\n","\n","def convert_to_tensor(x, encoded_dtype):\n","  if len(x) == 1:\n","    out = np.frombuffer(x[0].numpy(), dtype=encoded_dtype)\n","  else:\n","    out = []\n","    for el in x:\n","      out.append(np.frombuffer(el.numpy(), dtype=encoded_dtype))\n","  out = tf.convert_to_tensor(np.array(out))\n","  return out\n","\n","\n","def parse_serialized_simulation_example(example_proto, metadata):\n","  \"\"\"Parses a serialized simulation tf.SequenceExample.\n","\n","  Args:\n","    example_proto: A string encoding of the tf.SequenceExample proto.\n","    metadata: A dict of metadata for the dataset.\n","\n","  Returns:\n","    context: A dict, with features that do not vary over the trajectory.\n","    parsed_features: A dict of tf.Tensors representing the parsed examples\n","      across time, where axis zero is the time axis.\n","\n","  \"\"\"\n","  if 'context_mean' in metadata:\n","    feature_description = _FEATURE_DESCRIPTION_WITH_GLOBAL_CONTEXT\n","  else:\n","    feature_description = _FEATURE_DESCRIPTION\n","  context, parsed_features = tf.io.parse_single_sequence_example(\n","      example_proto,\n","      context_features=_CONTEXT_FEATURES,\n","      sequence_features=feature_description)\n","  for feature_key, item in parsed_features.items():\n","    convert_fn = functools.partial(\n","        convert_to_tensor, encoded_dtype=_FEATURE_DTYPES[feature_key]['in'])\n","    parsed_features[feature_key] = tf.py_function(\n","        convert_fn, inp=[item.values], Tout=_FEATURE_DTYPES[feature_key]['out'])\n","\n","  # There is an extra frame at the beginning so we can calculate pos change\n","  # for all frames used in the paper.\n","  position_shape = [metadata['sequence_length'] + 1, -1, metadata['dim']]\n","\n","  # Reshape positions to correct dim:\n","  parsed_features['position'] = tf.reshape(parsed_features['position'],\n","                                           position_shape)\n","  # Set correct shapes of the remaining tensors.\n","  sequence_length = metadata['sequence_length'] + 1\n","  if 'context_mean' in metadata:\n","    context_feat_len = len(metadata['context_mean'])\n","    parsed_features['step_context'] = tf.reshape(\n","        parsed_features['step_context'],\n","        [sequence_length, context_feat_len])\n","  # Decode particle type explicitly\n","  context['particle_type'] = tf.py_function(\n","      functools.partial(convert_fn, encoded_dtype=np.int64),\n","      inp=[context['particle_type'].values],\n","      Tout=[tf.int64])\n","  context['particle_type'] = tf.reshape(context['particle_type'], [-1])\n","  return context, parsed_features\n","\n","\n","def split_trajectory(context, features, window_length=7):\n","  \"\"\"Splits trajectory into sliding windows.\"\"\"\n","  # Our strategy is to make sure all the leading dimensions are the same size,\n","  # then we can use from_tensor_slices.\n","\n","  trajectory_length = features['position'].get_shape().as_list()[0]\n","\n","  # We then stack window_length position changes so the final\n","  # trajectory length will be - window_length +1 (the 1 to make sure we get\n","  # the last split).\n","  input_trajectory_length = trajectory_length - window_length + 1\n","\n","  model_input_features = {}\n","  # Prepare the context features per step.\n","  model_input_features['particle_type'] = tf.tile(\n","      tf.expand_dims(context['particle_type'], axis=0),\n","      [input_trajectory_length, 1])\n","\n","  if 'step_context' in features:\n","    global_stack = []\n","    for idx in range(input_trajectory_length):\n","      global_stack.append(features['step_context'][idx:idx + window_length])\n","    model_input_features['step_context'] = tf.stack(global_stack)\n","\n","  pos_stack = []\n","  for idx in range(input_trajectory_length):\n","    pos_stack.append(features['position'][idx:idx + window_length])\n","  # Get the corresponding positions\n","  model_input_features['position'] = tf.stack(pos_stack)\n","\n","  return tf.data.Dataset.from_tensor_slices(model_input_features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"plvXvp6Yj94Y"},"source":["### graph connectivity"]},{"cell_type":"code","metadata":{"id":"76EE3VCqkCJG"},"source":["def _compute_connectivity(positions, radius):\n","  \"\"\"Get the indices of connected edges with radius connectivity.\n","\n","  Args:\n","    positions: Positions of nodes in the graph. Shape:\n","      [num_nodes_in_graph, num_dims].\n","    radius: Radius of connectivity.\n","\n","  Returns:\n","    senders indices [num_edges_in_graph]\n","    receiver indices [num_edges_in_graph]\n","\n","  \"\"\"\n","  tree = neighbors.KDTree(positions)\n","  receivers_list = tree.query_radius(positions, r=radius)\n","  num_nodes = len(positions)\n","  senders = np.repeat(range(num_nodes), [len(a) for a in receivers_list])\n","  receivers = np.concatenate(receivers_list, axis=0)\n","  return senders, receivers\n","\n","\n","def _compute_connectivity_for_batch(positions, n_node, radius):\n","  \"\"\"`compute_connectivity` for a batch of graphs.\n","\n","  Args:\n","    positions: Positions of nodes in the batch of graphs. Shape:\n","      [num_nodes_in_batch, num_dims].\n","    n_node: Number of nodes for each graph in the batch. Shape:\n","      [num_graphs in batch].\n","    radius: Radius of connectivity.\n","\n","  Returns:\n","    senders indices [num_edges_in_batch]\n","    receiver indices [num_edges_in_batch]\n","    number of edges per graph [num_graphs_in_batch]\n","\n","  \"\"\"\n","\n","  # TODO(alvarosg): Consider if we want to support batches here or not.\n","  # Separate the positions corresponding to particles in different graphs.\n","  positions_per_graph_list = np.split(positions, np.cumsum(n_node[:-1]), axis=0)\n","  receivers_list = []\n","  senders_list = []\n","  n_edge_list = []\n","  num_nodes_in_previous_graphs = 0\n","\n","  # Compute connectivity for each graph in the batch.\n","  for positions_graph_i in positions_per_graph_list:\n","    senders_graph_i, receivers_graph_i = _compute_connectivity(\n","        positions_graph_i, radius)\n","\n","    num_edges_graph_i = len(senders_graph_i)\n","    n_edge_list.append(num_edges_graph_i)\n","\n","    # Because the inputs will be concatenated, we need to add offsets to the\n","    # sender and receiver indices according to the number of nodes in previous\n","    # graphs in the same batch.\n","    receivers_list.append(receivers_graph_i + num_nodes_in_previous_graphs)\n","    senders_list.append(senders_graph_i + num_nodes_in_previous_graphs)\n","\n","    num_nodes_graph_i = len(positions_graph_i)\n","    num_nodes_in_previous_graphs += num_nodes_graph_i\n","\n","  # Concatenate all of the results.\n","  senders = np.concatenate(senders_list, axis=0).astype(np.int32)\n","  receivers = np.concatenate(receivers_list, axis=0).astype(np.int32)\n","  n_edge = np.stack(n_edge_list).astype(np.int32)\n","\n","  return senders, receivers, n_edge\n","\n","\n","def compute_connectivity_for_batch_pyfunc(positions, n_node, radius):\n","  \"\"\"`_compute_connectivity_for_batch` wrapped in a pyfunc.\"\"\"\n","  senders, receivers, n_edge = tf.py_function(\n","      _compute_connectivity_for_batch,\n","      [positions, n_node, radius], [tf.int32, tf.int32, tf.int32])\n","  senders.set_shape([None])\n","  receivers.set_shape([None])\n","  n_edge.set_shape(n_node.get_shape())\n","  return senders, receivers, n_edge"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ixV3hdvkGVv"},"source":["### noise"]},{"cell_type":"code","metadata":{"id":"Mx9oPAQlkJxL"},"source":["# Lint as: python3\n","# pylint: disable=g-bad-file-header\n","# Copyright 2020 DeepMind Technologies Limited. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#    http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ============================================================================\n","\"\"\"Methods to calculate input noise.\"\"\"\n","\n","import tensorflow.compat.v1 as tf\n","\n","\n","def get_random_walk_noise_for_position_sequence(\n","    position_sequence, noise_std_last_step):\n","  \"\"\"Returns random-walk noise in the velocity applied to the position.\"\"\"\n","\n","  velocity_sequence = time_diff(position_sequence)\n","\n","  # We want the noise scale in the velocity at the last step to be fixed.\n","  # Because we are going to compose noise at each step using a random_walk:\n","  # std_last_step**2 = num_velocities * std_each_step**2\n","  # so to keep `std_last_step` fixed, we apply at each step:\n","  # std_each_step `std_last_step / np.sqrt(num_input_velocities)`\n","  # TODO(alvarosg): Make sure this is consistent with the value and\n","  # description provided in the paper.\n","  num_velocities = velocity_sequence.shape.as_list()[1]\n","  velocity_sequence_noise = tf.random.normal(\n","      tf.shape(velocity_sequence),\n","      stddev=noise_std_last_step / num_velocities ** 0.5,\n","      dtype=position_sequence.dtype)\n","\n","  # Apply the random walk.\n","  velocity_sequence_noise = tf.cumsum(velocity_sequence_noise, axis=1)\n","\n","  # Integrate the noise in the velocity to the positions, assuming\n","  # an Euler intergrator and a dt = 1, and adding no noise to the very first\n","  # position (since that will only be used to calculate the first position\n","  # change).\n","  position_sequence_noise = tf.concat([\n","      tf.zeros_like(velocity_sequence_noise[:, 0:1]),\n","      tf.cumsum(velocity_sequence_noise, axis=1)], axis=1)\n","\n","  return position_sequence_noise\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7dxwqHRCMG9"},"source":["# Graph Network"]},{"cell_type":"code","metadata":{"id":"S8wI6GD5kYzX"},"source":["def build_mlp(\n","    hidden_size: int, num_hidden_layers: int, output_size: int) -> snt.Module:\n","  \"\"\"Builds an MLP.\"\"\"\n","  return snt.nets.MLP(\n","      output_sizes=[hidden_size] * num_hidden_layers + [output_size])\n","\n","\n","class EncodeProcessDecode(snt.AbstractModule):\n","  \"\"\"Encode-Process-Decode function approximator for learnable simulator.\"\"\"\n","\n","  def __init__(\n","      self,\n","      latent_size: int,\n","      mlp_hidden_size: int,\n","      mlp_num_hidden_layers: int,\n","      num_message_passing_steps: int,\n","      output_size: int,\n","      name: str = \"EncodeProcessDecode\"):\n","    \"\"\"Inits the model.\n","\n","    Args:\n","      latent_size: Size of the node and edge latent representations.\n","      mlp_hidden_size: Hidden layer size for all MLPs.\n","      mlp_num_hidden_layers: Number of hidden layers in all MLPs.\n","      num_message_passing_steps: Number of message passing steps.\n","      output_size: Output size of the decode node representations as required\n","        by the downstream update function.\n","      name: Name of the model.\n","    \"\"\"\n","\n","    super().__init__(name=name)\n","\n","    self._latent_size = latent_size\n","    self._mlp_hidden_size = mlp_hidden_size\n","    self._mlp_num_hidden_layers = mlp_num_hidden_layers\n","    self._num_message_passing_steps = num_message_passing_steps\n","    self._output_size = output_size\n","    self._residual_stride = 2\n","    with self._enter_variable_scope():\n","      self._networks_builder()\n","\n","  def _build(self, input_graph: gn.graphs.GraphsTuple) -> tf.Tensor:\n","    \"\"\"Forward pass of the learnable dynamics model.\"\"\"\n","\n","    # Encode the input_graph.\n","    latent_graph_0 = self._encode(input_graph)\n","\n","    # Do `m` message passing steps in the latent graphs.\n","    latent_graph_m = self._process(latent_graph_0)\n","\n","    # Decode from the last latent graph.\n","    return self._decode(latent_graph_m)\n","\n","  def _networks_builder(self):\n","    \"\"\"Builds the networks.\"\"\"\n","\n","    def build_mlp_with_layer_norm():\n","      mlp = build_mlp(\n","          hidden_size=self._mlp_hidden_size,\n","          num_hidden_layers=self._mlp_num_hidden_layers,\n","          output_size=self._latent_size)\n","      return snt.Sequential([mlp, snt.LayerNorm()])\n","\n","    # The encoder graph network independently encodes edge and node features.\n","    encoder_kwargs = dict(\n","        edge_model_fn=build_mlp_with_layer_norm,\n","        node_model_fn=build_mlp_with_layer_norm)\n","    self._encoder_network = gn.modules.GraphIndependent(**encoder_kwargs)\n","\n","    # Create `num_message_passing_steps` graph networks with unshared parameters\n","    # that update the node and edge latent features.\n","    # Note that we can use `modules.InteractionNetwork` because\n","    # it also outputs the messages as updated edge latent features.\n","    self._processor_networks = []\n","    for _ in range(self._num_message_passing_steps):\n","      self._processor_networks.append(\n","          gn.modules.InteractionNetwork(\n","              edge_model_fn=build_mlp_with_layer_norm,\n","              node_model_fn=build_mlp_with_layer_norm))\n","\n","    # The decoder MLP decodes node latent features into the output size.\n","    self._decoder_network = build_mlp(\n","        hidden_size=self._mlp_hidden_size,\n","        num_hidden_layers=self._mlp_num_hidden_layers,\n","        output_size=self._output_size)\n","\n","  def _encode(\n","      self, input_graph: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n","    \"\"\"Encodes the input graph features into a latent graph.\"\"\"\n","\n","    # Copy the globals to all of the nodes, if applicable.\n","    if input_graph.globals is not None:\n","      broadcasted_globals = gn.blocks.broadcast_globals_to_nodes(input_graph)\n","      input_graph = input_graph.replace(\n","          nodes=tf.concat([input_graph.nodes, broadcasted_globals], axis=-1),\n","          globals=None)\n","\n","    # Encode the node and edge features.\n","    latent_graph_0 = self._encoder_network(input_graph)\n","    return latent_graph_0\n","\n","  def _process_residual(\n","      self, latent_graph_0: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n","      stride = self._residual_stride\n","      latent_graph = latent_graph_0\n","      latent_graph_prev = latent_graph\n","      for step, processor in enumerate(self._processor_networks):\n","        latent_graph = processor(latent_graph)\n","        if step % stride == 0:\n","          if step != 0:\n","            latent_graph = latent_graph.replace(\n","                nodes=latent_graph.nodes+latent_graph_prev.nodes\n","                ,edges=latent_graph.edges+latent_graph_prev.edges)\n","          latent_graph_prev = latent_graph\n","      return latent_graph\n","\n","  def _process(\n","      self, latent_graph_0: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n","    \"\"\"Processes the latent graph with several steps of message passing.\"\"\"\n","\n","    # Do `m` message passing steps in the latent graphs.\n","    # (In the shared parameters case, just reuse the same `processor_network`)\n","    latent_graph_prev_k = latent_graph_0\n","    for processor_network_k in self._processor_networks:\n","      latent_graph_k = self._process_step(\n","          processor_network_k, latent_graph_prev_k)\n","      latent_graph_prev_k = latent_graph_k\n","\n","    latent_graph_m = latent_graph_k\n","    return latent_graph_m\n","\n","  def _process_step(\n","      self, processor_network_k: snt.Module,\n","      latent_graph_prev_k: gn.graphs.GraphsTuple) -> gn.graphs.GraphsTuple:\n","    \"\"\"Single step of message passing with node/edge residual connections.\"\"\"\n","\n","    # One step of message passing.\n","    latent_graph_k = processor_network_k(latent_graph_prev_k)\n","\n","    # Add residuals.\n","    latent_graph_k = latent_graph_k.replace(\n","        nodes=latent_graph_k.nodes+latent_graph_prev_k.nodes,\n","        edges=latent_graph_k.edges+latent_graph_prev_k.edges)\n","    return latent_graph_k\n","\n","  def _decode(self, latent_graph: gn.graphs.GraphsTuple) -> tf.Tensor:\n","    \"\"\"Decodes from the latent graph.\"\"\"\n","    return self._decoder_network(latent_graph.nodes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvqLj_tfCTaU"},"source":["# Wrapper Class (pre & post processing)"]},{"cell_type":"code","metadata":{"id":"yFsA-EiEkgsW"},"source":["STD_EPSILON = 1e-8\n","\n","\n","class LearnedSimulator(snt.AbstractModule):\n","  \"\"\"Learned simulator from https://arxiv.org/pdf/2002.09405.pdf.\"\"\"\n","\n","  def __init__(\n","      self,\n","      num_dimensions,\n","      connectivity_radius,\n","      graph_network_kwargs,\n","      boundaries,\n","      normalization_stats,\n","      num_particle_types,\n","      particle_type_embedding_size,\n","      name=\"LearnedSimulator\"):\n","    \"\"\"Inits the model.\n","\n","    Args:\n","      num_dimensions: Dimensionality of the problem.\n","      connectivity_radius: Scalar with the radius of connectivity.\n","      graph_network_kwargs: Keyword arguments to pass to the learned part\n","        of the graph network `model.EncodeProcessDecode`.\n","      boundaries: List of 2-tuples, containing the lower and upper boundaries of\n","        the cuboid containing the particles along each dimensions, matching\n","        the dimensionality of the problem.\n","      normalization_stats: Dictionary with statistics with keys \"acceleration\"\n","        and \"velocity\", containing a named tuple for each with mean and std\n","        fields, matching the dimensionality of the problem.\n","      num_particle_types: Number of different particle types.\n","      particle_type_embedding_size: Embedding size for the particle type.\n","      name: Name of the Sonnet module.\n","\n","    \"\"\"\n","    super().__init__(name=name)\n","\n","    self._connectivity_radius = connectivity_radius\n","    self._num_particle_types = num_particle_types\n","    self._boundaries = boundaries\n","    self._normalization_stats = normalization_stats\n","    with self._enter_variable_scope():\n","      self._graph_network = EncodeProcessDecode(\n","          output_size=num_dimensions, **graph_network_kwargs)\n","\n","      if self._num_particle_types > 1:\n","        self._particle_type_embedding = tf.get_variable(\n","            \"particle_embedding\",\n","            [self._num_particle_types, particle_type_embedding_size],\n","            trainable=True, use_resource=True)\n","\n","  def _build(self, position_sequence, n_particles_per_example,\n","             global_context=None, particle_types=None):\n","    \"\"\"Produces a model step, outputting the next position for each particle.\n","\n","    Args:\n","      position_sequence: Sequence of positions for each node in the batch,\n","        with shape [num_particles_in_batch, sequence_length, num_dimensions]\n","      n_particles_per_example: Number of particles for each graph in the batch\n","        with shape [batch_size]\n","      global_context: Tensor of shape [batch_size, context_size], with global\n","        context.\n","      particle_types: Integer tensor of shape [num_particles_in_batch] with\n","        the integer types of the particles, from 0 to `num_particle_types - 1`.\n","        If None, we assume all particles are the same type.\n","\n","    Returns:\n","      Next position with shape [num_particles_in_batch, num_dimensions] for one\n","      step into the future from the input sequence.\n","    \"\"\"\n","    input_graphs_tuple = self._encoder_preprocessor(\n","        position_sequence, n_particles_per_example, global_context,\n","        particle_types)\n","\n","    normalized_acceleration = self._graph_network(input_graphs_tuple)\n","\n","    next_position = self._decoder_postprocessor(\n","        normalized_acceleration, position_sequence)\n","\n","    return next_position\n","\n","  def _encoder_preprocessor(\n","      self, position_sequence, n_node, global_context, particle_types):\n","    # Extract important features from the position_sequence.\n","    most_recent_position = position_sequence[:, -1]\n","    velocity_sequence = time_diff(position_sequence)  # Finite-difference.\n","\n","    # Get connectivity of the graph.\n","    (senders, receivers, n_edge\n","     ) = compute_connectivity_for_batch_pyfunc(\n","         most_recent_position, n_node, self._connectivity_radius)\n","\n","    # Collect node features.\n","    node_features = []\n","\n","    # Normalized velocity sequence, merging spatial an time axis.\n","    velocity_stats = self._normalization_stats[\"velocity\"]\n","    normalized_velocity_sequence = (\n","        velocity_sequence - velocity_stats.mean) / velocity_stats.std\n","\n","    flat_velocity_sequence = snt.MergeDims(start=1, size=2)(\n","        normalized_velocity_sequence)\n","    node_features.append(flat_velocity_sequence)\n","\n","    # Normalized clipped distances to lower and upper boundaries.\n","    # boundaries are an array of shape [num_dimensions, 2], where the second\n","    # axis, provides the lower/upper boundaries.\n","    boundaries = tf.constant(self._boundaries, dtype=tf.float32)\n","    distance_to_lower_boundary = (\n","        most_recent_position - tf.expand_dims(boundaries[:, 0], 0))\n","    distance_to_upper_boundary = (\n","        tf.expand_dims(boundaries[:, 1], 0) - most_recent_position)\n","    distance_to_boundaries = tf.concat(\n","        [distance_to_lower_boundary, distance_to_upper_boundary], axis=1)\n","    normalized_clipped_distance_to_boundaries = tf.clip_by_value(\n","        distance_to_boundaries / self._connectivity_radius, -1., 1.)\n","    node_features.append(normalized_clipped_distance_to_boundaries)\n","\n","    # Particle type.\n","    if self._num_particle_types > 1:\n","      particle_type_embeddings = tf.nn.embedding_lookup(\n","          self._particle_type_embedding, particle_types)\n","      node_features.append(particle_type_embeddings)\n","\n","    # Collect edge features.\n","    edge_features = []\n","\n","    # Relative displacement and distances normalized to radius\n","    normalized_relative_displacements = (\n","        tf.gather(most_recent_position, senders) -\n","        tf.gather(most_recent_position, receivers)) / self._connectivity_radius\n","    edge_features.append(normalized_relative_displacements)\n","\n","    normalized_relative_distances = tf.norm(\n","        normalized_relative_displacements, axis=-1, keepdims=True)\n","    edge_features.append(normalized_relative_distances)\n","\n","    # Normalize the global context.\n","    if global_context is not None:\n","      context_stats = self._normalization_stats[\"context\"]\n","      # Context in some datasets are all zero, so add an epsilon for numerical\n","      # stability.\n","      global_context = (global_context - context_stats.mean) / tf.math.maximum(\n","          context_stats.std, STD_EPSILON)\n","\n","    return gn.graphs.GraphsTuple(\n","        nodes=tf.concat(node_features, axis=-1),\n","        edges=tf.concat(edge_features, axis=-1),\n","        globals=global_context,  # self._graph_net will appending this to nodes.\n","        n_node=n_node,\n","        n_edge=n_edge,\n","        senders=senders,\n","        receivers=receivers,\n","        )\n","\n","  def _decoder_postprocessor(self, normalized_acceleration, position_sequence):\n","\n","    # The model produces the output in normalized space so we apply inverse\n","    # normalization.\n","    acceleration_stats = self._normalization_stats[\"acceleration\"]\n","    acceleration = (\n","        normalized_acceleration * acceleration_stats.std\n","        ) + acceleration_stats.mean\n","\n","    # Use an Euler integrator to go from acceleration to position, assuming\n","    # a dt=1 corresponding to the size of the finite difference.\n","    most_recent_position = position_sequence[:, -1]\n","    most_recent_velocity = most_recent_position - position_sequence[:, -2]\n","\n","    new_velocity = most_recent_velocity + acceleration  # * dt = 1\n","    new_position = most_recent_position + new_velocity  # * dt = 1\n","    return new_position\n","\n","  def get_predicted_and_target_normalized_accelerations(\n","      self, next_position, position_sequence_noise, position_sequence,\n","      n_particles_per_example, global_context=None, particle_types=None):  # pylint: disable=g-doc-args\n","    \"\"\"Produces normalized and predicted acceleration targets.\n","\n","    Args:\n","      next_position: Tensor of shape [num_particles_in_batch, num_dimensions]\n","        with the positions the model should output given the inputs.\n","      position_sequence_noise: Tensor of the same shape as `position_sequence`\n","        with the noise to apply to each particle.\n","      position_sequence, n_node, global_context, particle_types: Inputs to the\n","        model as defined by `_build`.\n","\n","    Returns:\n","      Tensors of shape [num_particles_in_batch, num_dimensions] with the\n","        predicted and target normalized accelerations.\n","    \"\"\"\n","\n","    # Add noise to the input position sequence.\n","    noisy_position_sequence = position_sequence + position_sequence_noise\n","\n","    # Perform the forward pass with the noisy position sequence.\n","    input_graphs_tuple = self._encoder_preprocessor(\n","        noisy_position_sequence, n_particles_per_example, global_context,\n","        particle_types)\n","    predicted_normalized_acceleration = self._graph_network(input_graphs_tuple)\n","\n","    # Calculate the target acceleration, using an `adjusted_next_position `that\n","    # is shifted by the noise in the last input position.\n","    next_position_adjusted = next_position + position_sequence_noise[:, -1]\n","    target_normalized_acceleration = self._inverse_decoder_postprocessor(\n","        next_position_adjusted, noisy_position_sequence)\n","    # As a result the inverted Euler update in the `_inverse_decoder` produces:\n","    # * A target acceleration that does not explicitly correct for the noise in\n","    #   the input positions, as the `next_position_adjusted` is different\n","    #   from the true `next_position`.\n","    # * A target acceleration that exactly corrects noise in the input velocity\n","    #   since the target next velocity calculated by the inverse Euler update\n","    #   as `next_position_adjusted - noisy_position_sequence[:,-1]`\n","    #   matches the ground truth next velocity (noise cancels out).\n","\n","    return predicted_normalized_acceleration, target_normalized_acceleration\n","\n","  def _inverse_decoder_postprocessor(self, next_position, position_sequence):\n","    \"\"\"Inverse of `_decoder_postprocessor`.\"\"\"\n","\n","    previous_position = position_sequence[:, -1]\n","    previous_velocity = previous_position - position_sequence[:, -2]\n","    next_velocity = next_position - previous_position\n","    acceleration = next_velocity - previous_velocity\n","\n","    acceleration_stats = self._normalization_stats[\"acceleration\"]\n","    normalized_acceleration = (\n","        acceleration - acceleration_stats.mean) / acceleration_stats.std\n","    return normalized_acceleration\n","\n","\n","def time_diff(input_sequence):\n","  return input_sequence[:, 1:] - input_sequence[:, :-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olQE4IXyCed4"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"okg2l1Xclfze"},"source":["### helper functions\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"3H7gVAysk8WX","executionInfo":{"status":"ok","timestamp":1607971583171,"user_tz":480,"elapsed":1553,"user":{"displayName":"Xiao Liang","photoUrl":"","userId":"15028243839916116675"}},"outputId":"1e0a871d-886f-499f-ec7f-172222b6cd1b"},"source":["\n","Stats = collections.namedtuple('Stats', ['mean', 'std'])\n","\n","INPUT_SEQUENCE_LENGTH = 6  # So we can calculate the last 5 velocities.\n","NUM_PARTICLE_TYPES = 9\n","KINEMATIC_PARTICLE_ID = 3\n","\n","\n","def get_kinematic_mask(particle_types):\n","  \"\"\"Returns a boolean mask, set to true for kinematic (obstacle) particles.\"\"\"\n","  return tf.equal(particle_types, KINEMATIC_PARTICLE_ID)\n","\n","\n","def prepare_inputs(tensor_dict):\n","  \"\"\"Prepares a single stack of inputs by calculating inputs and targets.\n","\n","  Computes n_particles_per_example, which is a tensor that contains information\n","  about how to partition the axis - i.e. which nodes belong to which graph.\n","\n","  Adds a batch axis to `n_particles_per_example` and `step_context` so they can\n","  later be batched using `batch_concat`. This batch will be the same as if the\n","  elements had been batched via stacking.\n","\n","  Note that all other tensors have a variable size particle axis,\n","  and in this case they will simply be concatenated along that\n","  axis.\n","\n","\n","\n","  Args:\n","    tensor_dict: A dict of tensors containing positions, and step context (\n","    if available).\n","\n","  Returns:\n","    A tuple of input features and target positions.\n","\n","  \"\"\"\n","  # Position is encoded as [sequence_length, num_particles, dim] but the model\n","  # expects [num_particles, sequence_length, dim].\n","  pos = tensor_dict['position']\n","  pos = tf.transpose(pos, perm=[1, 0, 2])\n","\n","  # The target position is the final step of the stack of positions.\n","  target_position = pos[:, -1]\n","\n","  # Remove the target from the input.\n","  tensor_dict['position'] = pos[:, :-1]\n","\n","  # Compute the number of particles per example.\n","  num_particles = tf.shape(pos)[0]\n","  # Add an extra dimension for stacking via concat.\n","  tensor_dict['n_particles_per_example'] = num_particles[tf.newaxis]\n","\n","  if 'step_context' in tensor_dict:\n","    # Take the input global context. We have a stack of global contexts,\n","    # and we take the penultimate since the final is the target.\n","    tensor_dict['step_context'] = tensor_dict['step_context'][-2]\n","    # Add an extra dimension for stacking via concat.\n","    tensor_dict['step_context'] = tensor_dict['step_context'][tf.newaxis]\n","  return tensor_dict, target_position\n","\n","\n","def prepare_rollout_inputs(context, features):\n","  \"\"\"Prepares an inputs trajectory for rollout.\"\"\"\n","  out_dict = {**context}\n","  # Position is encoded as [sequence_length, num_particles, dim] but the model\n","  # expects [num_particles, sequence_length, dim].\n","  pos = tf.transpose(features['position'], [1, 0, 2])\n","  # The target position is the final step of the stack of positions.\n","  target_position = pos[:, -1]\n","  # Remove the target from the input.\n","  out_dict['position'] = pos[:, :-1]\n","  # Compute the number of nodes\n","  out_dict['n_particles_per_example'] = [tf.shape(pos)[0]]\n","  if 'step_context' in features:\n","    out_dict['step_context'] = features['step_context']\n","  out_dict['is_trajectory'] = tf.constant([True], tf.bool)\n","  return out_dict, target_position\n","\n","\n","def batch_concat(dataset, batch_size):\n","  \"\"\"We implement batching as concatenating on the leading axis.\"\"\"\n","\n","  # We create a dataset of datasets of length batch_size.\n","  windowed_ds = dataset.window(batch_size)\n","\n","  # The plan is then to reduce every nested dataset by concatenating. We can\n","  # do this using tf.data.Dataset.reduce. This requires an initial state, and\n","  # then incrementally reduces by running through the dataset\n","\n","  # Get initial state. In this case this will be empty tensors of the\n","  # correct shape.\n","  initial_state = tree.map_structure(\n","      lambda spec: tf.zeros(  # pylint: disable=g-long-lambda\n","          shape=[0] + spec.shape.as_list()[1:], dtype=spec.dtype),\n","      dataset.element_spec)\n","\n","  # We run through the nest and concatenate each entry with the previous state.\n","  def reduce_window(initial_state, ds):\n","    return ds.reduce(initial_state, lambda x, y: tf.concat([x, y], axis=0))\n","\n","  return windowed_ds.map(\n","      lambda *x: tree.map_structure(reduce_window, initial_state, x))\n","\n","\n","def get_input_fn(data_path, batch_size, mode, split):\n","  \"\"\"Gets the learning simulation input function for tf.estimator.Estimator.\n","\n","  Args:\n","    data_path: the path to the dataset directory.\n","    batch_size: the number of graphs in a batch.\n","    mode: either 'one_step_train', 'one_step' or 'rollout'\n","    split: either 'train', 'valid' or 'test.\n","\n","  Returns:\n","    The input function for the learning simulation model.\n","  \"\"\"\n","  def input_fn():\n","    \"\"\"Input function for learning simulation.\"\"\"\n","    # Loads the metadata of the dataset.\n","    metadata = _read_metadata(data_path)\n","    # Create a tf.data.Dataset from the TFRecord.\n","    ds = tf.data.TFRecordDataset([os.path.join(data_path, f'{split}.tfrecord')])\n","    ds = ds.map(functools.partial(parse_serialized_simulation_example, metadata=metadata))\n","    if mode.startswith('one_step'):\n","      # Splits an entire trajectory into chunks of 7 steps.\n","      # Previous 5 velocities, current velocity and target.\n","      split_with_window = functools.partial(\n","          split_trajectory,\n","          window_length=INPUT_SEQUENCE_LENGTH + 1)\n","      ds = ds.flat_map(split_with_window)\n","      # Splits a chunk into input steps and target steps\n","      ds = ds.map(prepare_inputs)\n","      # If in train mode, repeat dataset forever and shuffle.\n","      if mode == 'one_step_train':\n","        ds = ds.repeat()\n","        ds = ds.shuffle(512)\n","      # Custom batching on the leading axis.\n","      ds = batch_concat(ds, batch_size)\n","    elif mode == 'rollout':\n","      # Rollout evaluation only available for batch size 1\n","      assert batch_size == 1\n","      ds = ds.map(prepare_rollout_inputs)\n","    else:\n","      raise ValueError(f'mode: {mode} not recognized')\n","    return ds\n","\n","  return input_fn\n","\n","\n","def rollout(simulator, features, num_steps):\n","  \"\"\"Rolls out a trajectory by applying the model in sequence.\"\"\"\n","  initial_positions = features['position'][:, 0:INPUT_SEQUENCE_LENGTH]\n","  ground_truth_positions = features['position'][:, INPUT_SEQUENCE_LENGTH:]\n","  global_context = features.get('step_context')\n","  def step_fn(step, current_positions, predictions):\n","\n","    if global_context is None:\n","      global_context_step = None\n","    else:\n","      global_context_step = global_context[\n","          step + INPUT_SEQUENCE_LENGTH - 1][tf.newaxis]\n","\n","    next_position = simulator(\n","        current_positions,\n","        n_particles_per_example=features['n_particles_per_example'],\n","        particle_types=features['particle_type'],\n","        global_context=global_context_step)\n","\n","    # Update kinematic particles from prescribed trajectory.\n","    kinematic_mask = get_kinematic_mask(features['particle_type'])\n","    next_position_ground_truth = ground_truth_positions[:, step]\n","    next_position = tf.where(kinematic_mask, next_position_ground_truth,\n","                             next_position)\n","    updated_predictions = predictions.write(step, next_position)\n","\n","    # Shift `current_positions`, removing the oldest position in the sequence\n","    # and appending the next position at the end.\n","    next_positions = tf.concat([current_positions[:, 1:],\n","                                next_position[:, tf.newaxis]], axis=1)\n","\n","    return (step + 1, next_positions, updated_predictions)\n","\n","  predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n","  _, _, predictions = tf.while_loop(\n","      cond=lambda step, state, prediction: tf.less(step, num_steps),\n","      body=step_fn,\n","      loop_vars=(0, initial_positions, predictions),\n","      back_prop=False,\n","      parallel_iterations=1)\n","\n","  output_dict = {\n","      'initial_positions': tf.transpose(initial_positions, [1, 0, 2]),\n","      'predicted_rollout': predictions.stack(),\n","      'ground_truth_rollout': tf.transpose(ground_truth_positions, [1, 0, 2]),\n","      'particle_types': features['particle_type'],\n","  }\n","\n","  if global_context is not None:\n","    output_dict['global_context'] = global_context\n","  return output_dict\n","\n","\n","def _combine_std(std_x, std_y):\n","  return np.sqrt(std_x**2 + std_y**2)\n","\n","\n","def _get_simulator(model_kwargs, metadata, acc_noise_std, vel_noise_std):\n","  \"\"\"Instantiates the simulator.\"\"\"\n","  # Cast statistics to numpy so they are arrays when entering the model.\n","  cast = lambda v: np.array(v, dtype=np.float32)\n","  acceleration_stats = Stats(\n","      cast(metadata['acc_mean']),\n","      _combine_std(cast(metadata['acc_std']), acc_noise_std))\n","  velocity_stats = Stats(\n","      cast(metadata['vel_mean']),\n","      _combine_std(cast(metadata['vel_std']), vel_noise_std))\n","  normalization_stats = {'acceleration': acceleration_stats,\n","                         'velocity': velocity_stats}\n","  if 'context_mean' in metadata:\n","    context_stats = Stats(\n","        cast(metadata['context_mean']), cast(metadata['context_std']))\n","    normalization_stats['context'] = context_stats\n","\n","  simulator = LearnedSimulator(\n","      num_dimensions=metadata['dim'],\n","      connectivity_radius=metadata['default_connectivity_radius'],\n","      graph_network_kwargs=model_kwargs,\n","      boundaries=metadata['bounds'],\n","      num_particle_types=NUM_PARTICLE_TYPES,\n","      normalization_stats=normalization_stats,\n","      particle_type_embedding_size=16)\n","  return simulator\n","\n","\n","def get_one_step_estimator_fn(data_path,\n","                              noise_std,\n","                              latent_size=128,\n","                              hidden_size=128,\n","                              hidden_layers=2,\n","                              message_passing_steps=10):\n","  \"\"\"Gets one step model for training simulation.\"\"\"\n","  metadata = _read_metadata(data_path)\n","\n","  model_kwargs = dict(\n","      latent_size=latent_size,\n","      mlp_hidden_size=hidden_size,\n","      mlp_num_hidden_layers=hidden_layers,\n","      num_message_passing_steps=message_passing_steps)\n","\n","  def estimator_fn(features, labels, mode):\n","    target_next_position = labels\n","    simulator = _get_simulator(model_kwargs, metadata,\n","                               vel_noise_std=noise_std,\n","                               acc_noise_std=noise_std)\n","    # Sample the noise to add to the inputs to the model during training.\n","    sampled_noise = get_random_walk_noise_for_position_sequence(\n","        features['position'], noise_std_last_step=noise_std)\n","    non_kinematic_mask = tf.logical_not(\n","        get_kinematic_mask(features['particle_type']))\n","    noise_mask = tf.cast(\n","        non_kinematic_mask, sampled_noise.dtype)[:, tf.newaxis, tf.newaxis]\n","    sampled_noise *= noise_mask\n","\n","    # Get the predictions and target accelerations.\n","    pred_target = simulator.get_predicted_and_target_normalized_accelerations(\n","        next_position=target_next_position,\n","        position_sequence=features['position'],\n","        position_sequence_noise=sampled_noise,\n","        n_particles_per_example=features['n_particles_per_example'],\n","        particle_types=features['particle_type'],\n","        global_context=features.get('step_context'))\n","    pred_acceleration, target_acceleration = pred_target\n","\n","    # Calculate the loss and mask out loss on kinematic particles/\n","    loss = (pred_acceleration - target_acceleration)**2\n","\n","    num_non_kinematic = tf.reduce_sum(\n","        tf.cast(non_kinematic_mask, tf.float32))\n","    loss = tf.where(non_kinematic_mask, loss, tf.zeros_like(loss))\n","    loss = tf.reduce_sum(loss) / tf.reduce_sum(num_non_kinematic)\n","    global_step = tf.train.get_global_step()\n","    # Set learning rate to decay from 1e-4 to 1e-6 exponentially.\n","    min_lr = 1e-6\n","    lr = tf.train.exponential_decay(learning_rate=1e-4 - min_lr,\n","                                    global_step=global_step,\n","                                    decay_steps=int(5e6),\n","                                    decay_rate=0.1) + min_lr\n","    opt = tf.train.AdamOptimizer(learning_rate=lr)\n","    train_op = opt.minimize(loss, global_step)\n","\n","    # Calculate next position and add some additional eval metrics (only eval).\n","    predicted_next_position = simulator(\n","        position_sequence=features['position'],\n","        n_particles_per_example=features['n_particles_per_example'],\n","        particle_types=features['particle_type'],\n","        global_context=features.get('step_context'))\n","\n","    predictions = {'predicted_next_position': predicted_next_position}\n","\n","    eval_metrics_ops = {\n","        'loss_mse': tf.metrics.mean_squared_error(\n","            pred_acceleration, target_acceleration),\n","        'one_step_position_mse': tf.metrics.mean_squared_error(\n","            predicted_next_position, target_next_position)\n","    }\n","    return tf.estimator.EstimatorSpec(\n","        mode=mode,\n","        train_op=train_op,\n","        loss=loss,\n","        predictions=predictions,\n","        eval_metric_ops=eval_metrics_ops)\n","\n","  return estimator_fn\n","\n","\n","def get_rollout_estimator_fn(data_path,\n","                             noise_std,\n","                             latent_size=128,\n","                             hidden_size=128,\n","                             hidden_layers=2,\n","                             message_passing_steps=10):\n","  \"\"\"Gets the model function for tf.estimator.Estimator.\"\"\"\n","  metadata = _read_metadata(data_path)\n","\n","  model_kwargs = dict(\n","      latent_size=latent_size,\n","      mlp_hidden_size=hidden_size,\n","      mlp_num_hidden_layers=hidden_layers,\n","      num_message_passing_steps=message_passing_steps)\n","\n","  def estimator_fn(features, labels, mode):\n","    del labels  # Labels to conform to estimator spec.\n","    simulator = _get_simulator(model_kwargs, metadata,\n","                               acc_noise_std=noise_std,\n","                               vel_noise_std=noise_std)\n","\n","    num_steps = metadata['sequence_length'] - INPUT_SEQUENCE_LENGTH\n","    rollout_op = rollout(simulator, features, num_steps=num_steps)\n","    squared_error = (rollout_op['predicted_rollout'] -\n","                     rollout_op['ground_truth_rollout']) ** 2\n","    loss = tf.reduce_mean(squared_error)\n","    eval_ops = {'rollout_error_mse': tf.metrics.mean_squared_error(\n","        rollout_op['predicted_rollout'], rollout_op['ground_truth_rollout'])}\n","\n","    # Add a leading axis, since Estimator's predict method insists that all\n","    # tensors have a shared leading batch axis fo the same dims.\n","    rollout_op = tree.map_structure(lambda x: x[tf.newaxis], rollout_op)\n","    return tf.estimator.EstimatorSpec(\n","        mode=mode,\n","        train_op=None,\n","        loss=loss,\n","        predictions=rollout_op,\n","        eval_metric_ops=eval_ops)\n","\n","  return estimator_fn\n","\n","\n","def _read_metadata(data_path):\n","  with open(os.path.join(data_path, 'metadata.json'), 'rt') as fp:\n","    return json.loads(fp.read())\n","\n","'''\n","def main(_):\n","  \"\"\"Train or evaluates the model.\"\"\"\n","\n","  if FLAGS.mode in ['train', 'eval']:\n","    estimator = tf.estimator.Estimator(\n","        get_one_step_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\n","        model_dir=FLAGS.model_path)\n","    if FLAGS.mode == 'train':\n","      # Train all the way through.\n","      estimator.train(\n","          input_fn=get_input_fn(FLAGS.data_path, FLAGS.batch_size,\n","                                mode='one_step_train', split='train'),\n","          max_steps=FLAGS.num_steps)\n","    else:\n","      # One-step evaluation from checkpoint.\n","      eval_metrics = estimator.evaluate(input_fn=get_input_fn(\n","          FLAGS.data_path, FLAGS.batch_size,\n","          mode='one_step', split=FLAGS.eval_split))\n","      logging.info('Evaluation metrics:')\n","      logging.info(eval_metrics)\n","  elif FLAGS.mode == 'eval_rollout':\n","    if not FLAGS.output_path:\n","      raise ValueError('A rollout path must be provided.')\n","    rollout_estimator = tf.estimator.Estimator(\n","        get_rollout_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\n","        model_dir=FLAGS.model_path)\n","\n","    # Iterate through rollouts saving them one by one.\n","    metadata = _read_metadata(FLAGS.data_path)\n","    rollout_iterator = rollout_estimator.predict(\n","        input_fn=get_input_fn(FLAGS.data_path, batch_size=1,\n","                              mode='rollout', split=FLAGS.eval_split))\n","\n","    for example_index, example_rollout in enumerate(rollout_iterator):\n","      example_rollout['metadata'] = metadata\n","      filename = f'rollout_{FLAGS.eval_split}_{example_index}.pkl'\n","      filename = os.path.join(FLAGS.output_path, filename)\n","      logging.info('Saving: %s.', filename)\n","      if not os.path.exists(FLAGS.output_path):\n","        os.mkdir(FLAGS.output_path)\n","      with open(filename, 'wb') as file:\n","        pickle.dump(example_rollout, file)\n","\n","if __name__ == '__main__':\n","  tf.disable_v2_behavior()\n","  app.run(main)\n","'''\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ndef main(_):\\n  \"\"\"Train or evaluates the model.\"\"\"\\n\\n  if FLAGS.mode in [\\'train\\', \\'eval\\']:\\n    estimator = tf.estimator.Estimator(\\n        get_one_step_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\\n        model_dir=FLAGS.model_path)\\n    if FLAGS.mode == \\'train\\':\\n      # Train all the way through.\\n      estimator.train(\\n          input_fn=get_input_fn(FLAGS.data_path, FLAGS.batch_size,\\n                                mode=\\'one_step_train\\', split=\\'train\\'),\\n          max_steps=FLAGS.num_steps)\\n    else:\\n      # One-step evaluation from checkpoint.\\n      eval_metrics = estimator.evaluate(input_fn=get_input_fn(\\n          FLAGS.data_path, FLAGS.batch_size,\\n          mode=\\'one_step\\', split=FLAGS.eval_split))\\n      logging.info(\\'Evaluation metrics:\\')\\n      logging.info(eval_metrics)\\n  elif FLAGS.mode == \\'eval_rollout\\':\\n    if not FLAGS.output_path:\\n      raise ValueError(\\'A rollout path must be provided.\\')\\n    rollout_estimator = tf.estimator.Estimator(\\n        get_rollout_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\\n        model_dir=FLAGS.model_path)\\n\\n    # Iterate through rollouts saving them one by one.\\n    metadata = _read_metadata(FLAGS.data_path)\\n    rollout_iterator = rollout_estimator.predict(\\n        input_fn=get_input_fn(FLAGS.data_path, batch_size=1,\\n                              mode=\\'rollout\\', split=FLAGS.eval_split))\\n\\n    for example_index, example_rollout in enumerate(rollout_iterator):\\n      example_rollout[\\'metadata\\'] = metadata\\n      filename = f\\'rollout_{FLAGS.eval_split}_{example_index}.pkl\\'\\n      filename = os.path.join(FLAGS.output_path, filename)\\n      logging.info(\\'Saving: %s.\\', filename)\\n      if not os.path.exists(FLAGS.output_path):\\n        os.mkdir(FLAGS.output_path)\\n      with open(filename, \\'wb\\') as file:\\n        pickle.dump(example_rollout, file)\\n\\nif __name__ == \\'__main__\\':\\n  tf.disable_v2_behavior()\\n  app.run(main)\\n'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"iSURALrClnmt"},"source":["### main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQGbvs1qlrnw","outputId":"c9d26ee1-ee12-4718-e2d5-6b13072ff525"},"source":["PROJECT_PATH = '/gdrive/MyDrive/CSE 490G/Final Project/'\n","data_path = PROJECT_PATH + 'data'\n","model_path = PROJECT_PATH + 'model'\n","num_steps = int(1e4)\n","noise_std = 6.7e-4\n","batch_size = 2\n","\n","\n","\n","estimator = tf.estimator.Estimator(\n","        get_one_step_estimator_fn(data_path, noise_std),\n","        model_dir=model_path)\n","# estimator.train(\n","#           input_fn=get_input_fn(data_path, batch_size,\n","#                                 mode='one_step_train', split='train'),\n","#           max_steps=num_steps)\n","\n","eval_metrics = estimator.evaluate(input_fn=get_input_fn(\n","    data_path, batch_size,\n","    mode='one_step', split='test'))\n","logging.info('Evaluation metrics:')\n","logging.info(eval_metrics)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","INFO:tensorflow:Using config: {'_model_dir': '/gdrive/MyDrive/CSE 490G/Final Project/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9d16e95cf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-12-14T18:57:55Z\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /gdrive/MyDrive/CSE 490G/Final Project/model/model.ckpt-11228\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DbCJodEygp50"},"source":["# Test and Eval"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":620},"id":"QgcrfMgxg-SV","executionInfo":{"status":"error","timestamp":1607925224957,"user_tz":480,"elapsed":3326,"user":{"displayName":"Xiao Liang","photoUrl":"","userId":"15028243839916116675"}},"outputId":"a9371004-3a0d-4cc9-8dd5-3744212cb2d2"},"source":["PROJECT_PATH = '/gdrive/MyDrive/CSE 490G/Final Project/'\n","rollout_path = PROJECT_PATH + 'rollouts'\n","data_path = PROJECT_PATH + 'data'\n","model_path = PROJECT_PATH + 'model'\n","num_steps = int(1e3)\n","noise_std = 6.7e-4\n","batch_size = 2\n","\n","rollout_estimator = tf.estimator.Estimator(\n","    get_rollout_estimator_fn(data_path, noise_std)\n","    ,model_dir=model_path)\n","\n","# Iterate through rollouts saving them one by one.\n","metadata = _read_metadata(data_path)\n","rollout_iterator = rollout_estimator.predict(input_fn=get_input_fn(data_path\n","                                                                   , batch_size=1\n","                                                                   ,mode='rollout'\n","                                                                   , split='test'))\n","\n","for example_index, example_rollout in enumerate(rollout_iterator):\n","  print(example_index)\n","  example_rollout['metadata'] = metadata\n","  filename = f'rollout_test_{example_index}.pkl'\n","  filename = os.path.join(rollout_path, filename)\n","  logging.info('Saving: %s.', filename)\n","  if not os.path.exists(rollout_path):\n","    os.mkdir(rollout_path)\n","  with open(filename, 'wb') as file:\n","    pickle.dump(example_rollout, file)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","INFO:tensorflow:Using config: {'_model_dir': '/gdrive/MyDrive/CSE 490G/Final Project/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9d6966b4e0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From <ipython-input-20-80f85109fb84>:174: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-0b1ae6a10f45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                                    , split='test'))\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mexample_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_rollout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mexample_rollout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    620\u001b[0m             input_fn, ModeKeys.PREDICT)\n\u001b[1;32m    621\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 622\u001b[0;31m             features, None, ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# Call to warm_start has to be after model_fn is called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-80f85109fb84>\u001b[0m in \u001b[0;36mestimator_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquared_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     eval_ops = {'rollout_error_mse': tf.metrics.mean_squared_error(\n\u001b[0;32m--> 344\u001b[0;31m         rollout_op['predicted_rollout'], rollout_op['ground_truth_rollout'])}\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# Add a leading axis, since Estimator's predict method insists that all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(labels, predictions, weights, metrics_collections, updates_collections, name)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0msquared_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquared_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m   return mean(squared_error, weights, metrics_collections, updates_collections,\n\u001b[0;32m-> 1323\u001b[0;31m               name or 'mean_squared_error')\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(values, weights, metrics_collections, updates_collections, name)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0mnum_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mupdate_total_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m       \u001b[0mupdate_count_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[0;34m(ref, value, use_locking, name)\u001b[0m\n\u001b[1;32m    192\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_ref_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     return gen_state_ops.assign_add(\n\u001b[0;32m--> 194\u001b[0;31m         ref, value, use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m    195\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[0;34m(ref, value, use_locking, name)\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0muse_locking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_locking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 114\u001b[0;31m         \"AssignAdd\", ref=ref, value=value, use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m    115\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    792\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    793\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;31m# Conditionally invoke tfdbg v2's op callback(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_in_graph_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIS_IN_GRAPH_MODE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_PRINT_DEPRECATION_WARNINGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0minvalid_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mnamed_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcallargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeprecated_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m           if (spec.position < len(args) and\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetcallargs\u001b[0;34m(*func_and_positional, **named)\u001b[0m\n\u001b[1;32m    276\u001b[0m   \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_positional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0mpositional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_positional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m   \u001b[0margspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m   \u001b[0mcall_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'im_self'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_maybe_argspec_to_fullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_getfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'return'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m         \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"OM23S0UslEwn"},"source":["### render trajectories"]},{"cell_type":"code","metadata":{"id":"b4BbACeJlD5y"},"source":["TYPE_TO_COLOR = {\n","    3: \"black\",  # Boundary particles.\n","    0: \"green\",  # Rigid solids.\n","    7: \"magenta\",  # Goop.\n","    6: \"gold\",  # Sand.\n","    5: \"blue\",  # Water.\n","}\n","\n","with open(FLAGS.rollout_path, \"rb\") as file:\n","    rollout_data = pickle.load(file)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","\n","plot_info = []\n","for ax_i, (label, rollout_field) in enumerate(\n","    [(\"Ground truth\", \"ground_truth_rollout\")\n","    ,(\"Prediction\", \"predicted_rollout\")]):\n","    # Append the initial positions to get the full trajectory.\n","  trajectory = np.concatenate([rollout_data[\"initial_positions\"]\n","                               ,rollout_data[rollout_field]], axis=0)\n","  ax = axes[ax_i]\n","  ax.set_title(label)\n","  bounds = rollout_data[\"metadata\"][\"bounds\"]\n","  ax.set_xlim(bounds[0][0], bounds[0][1])\n","  ax.set_ylim(bounds[1][0], bounds[1][1])\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  ax.set_aspect(1.)\n","  points = {\n","      particle_type: ax.plot([], [], \"o\", ms=2, color=color)[0]\n","      for particle_type, color in TYPE_TO_COLOR.items()}\n","  plot_info.append((ax, trajectory, points))\n","\n","num_steps = trajectory.shape[0]\n","\n","def update(step_i):\n","  outputs = []\n","  for _, trajectory, points in plot_info:\n","    for particle_type, line in points.items():\n","      mask = rollout_data[\"particle_types\"] == particle_type\n","      line.set_data(trajectory[step_i, mask, 0],\n","                    trajectory[step_i, mask, 1])\n","      outputs.append(line)\n","  return outputs\n","\n","unused_animation = animation.FuncAnimation(\n","    fig, update,\n","    frames=np.arange(0, num_steps, FLAGS.step_stride), interval=10)\n","plt.show(block=FLAGS.block_on_show)"],"execution_count":null,"outputs":[]}]}